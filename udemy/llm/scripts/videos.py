#!/usr/bin/env python
# coding: utf-8

# # Projeto 01 - Transcri√ß√£o e compreens√£o de v√≠deos
# 
# Neste projeto, vamos aprender a realizar transcri√ß√£o e compreens√£o de v√≠deos. Ao final, voc√™ ser√° capaz de criar sua pr√≥pria aplica√ß√£o que faz a sumariza√ß√£o autom√°tica de v√≠deos, permitindo que voc√™ entenda do que se trata e o que foi falado nele sem precisar assist√≠-lo.
# 
# Objetivos deste projeto:
# 
# * Compreender o conte√∫do de um v√≠deo do Youtube sem precisar assisti-lo.
# * Pesquisar informa√ß√µes √∫teis no v√≠deo sem perder nenhum detalhe importante.
# * Interagir com o conte√∫do do v√≠deo por meio de uma interface de chat (ou seja, como "conversar com o v√≠deo").
# 
# ## Instala√ß√£o e Configura√ß√£o
# 
# #!pip install -q langchain_core langchain_community langchain-huggingface langchain_ollama langchain_openai
# !pip install -q langchain_core==0.3.32 langchain_community==0.3.15 langchain-huggingface langchain_ollama langchain_openai
# 
# ### Instala√ß√£o de bibliotecas para baixar transcri√ß√£o
# 
# > **YouTube Transcript API**
# 
# Esta √© uma API python que permite que voc√™ obtenha a transcri√ß√£o/legendas para um determinado v√≠deo do YouTube. Ela tamb√©m funciona para legendas geradas automaticamente e possui suporta a uma fun√ß√£o que faz automaticamente a tradu√ß√£o de legendas
# 
# !pip install youtube-transcript-api==0.6.3
# 
# > **pytube**
# 
# Tamb√©m √© uma biblioteca que auxilia com o download de v√≠deos no youtube. Aqui ela n√£o √© necess√°ria para baixar as transcri√ß√µes dos v√≠deos, conseguimos ter acesso sem ela, mas iremos instalar tamb√©m pois com ela podemos recuperar tamb√©m demais informa√ß√µes do v√≠deo, como t√≠tulo, data de publica√ß√£o, descri√ß√£o, etc.
# 
# !pip install pytube
# 
# ## Importa√ß√µes
# 
# import os
# import io
# import getpass
# from langchain_community.document_loaders import YoutubeLoader
# from langchain_community.llms import HuggingFaceHub
# from langchain_openai import ChatOpenAI
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# 
# ## Carregando a transcri√ß√£o
# 
# Para fazer o carregamento das transcri√ß√µes usaremos o m√©todo YoutubeLoader(), que faz parte do dos document_loaders do LangChain. Veremos logo em seguida tamb√©m como extrair os metadados do v√≠deo usando essa fun√ß√£o
# 
# Atrav√©s desses m√©todo conseguimos puxar as transcri√ß√µes que j√° est√£o associadas ao v√≠deo e armazenadas no banco de dados do Youtube, o que ir√° nos economizar bastante processamento.
# 
# 
# O primeiro par√¢metro √© a URL do v√≠deo que queremos realizar a transcri√ß√£o
# 
# Vamos pegar esse v√≠deo como primeiro exemplo https://www.youtube.com/watch?v=II28i__Tf3M
# 
# ### Defininido idiomas
# 
# O segundo par√¢metro √© o language. A fun√ß√£o espera uma lista, nesse caso, uma lista de c√≥digos de idioma em prioridade decrescente (por padr√£o).
# 
# Al√©m de ingl√™s ("en"), recomendamos deixar antes "pt" e "pt-BR" (ou "pt-PT") pois em alguns v√≠deos n√£o possui "pt". Embora a grande maioria dos v√≠deos que testamos possua legenda com c√≥digo "pt", mesmo para v√≠deos com a legenda em portugu√™s brasileiro. Ou seja, deixamos assim pois em alguns v√≠deos do portugu√™s brasileiro por exemplo o c√≥digo √© "pt", j√° para outros est√° como "pt-BR".
# 
# 
# video_loader = YoutubeLoader.from_youtube_url("https://www.youtube.com/watch?v=II28i__Tf3M",
#                                               language = ["pt", "pt-BR", "en"],)
# 
# Usaremos o .load() para fazer a leitura e ao mesmo tempo podemos passar as informa√ß√µes do v√≠deo para uma vari√°vel
# 
# infos = video_loader.load()
# infos
# 
# O valor de "page_content" corresponde √† transcri√ß√£o em si
# 
# para acess√°-la devemos colocar `[0]` pois `infos` √© uma lista, que nesse caso s√≥ tem o primeiro valor. O c√≥digo ent√£o fica assim
# 
# transcricao = infos[0].page_content
# transcricao
# 
# Esse primeiro exemplo √© de uma legenda que foi gerada automaticamente pelo sistema de reconhecimento de fala do youtube, que no geral tende a ser bom mas pode gerar erros, ent√£o n√£o √© perfeito. Mas ainda assim, dependendo da LLM ela vai entender que se trata de um erro com base no contexto
# 
# Para legendas autom√°ticas verificamos que n√£o houve perda consider√°vel na compreens√£o, mas obviamento √© esperado que uma legenda feita manualmente possua maiores chances de resultados melhores
# 
# ### Obter informa√ß√µes do v√≠deo
# 
# Note que carregamos a legenda/transcri√ß√£o mas nenhuma outra informa√ß√£o sobre o v√≠deo, o que pode ser √∫til depedendo do nosso objetivo.
# 
# > H√° duas maneiras de obter informa√ß√µes do v√≠deo:
# 
# 1. Usar o par√¢metro `add_video_info` na fun√ß√£o YoutubeLoader.from_youtube_url(). Por tr√°s dos panos, a fun√ß√£o se comunica com a biblioteca pytube. No entanto, h√° um bug conhecido com o pytube que pode ocorrer ocasionalmente e que at√© o momento n√£o foi definitivamente resolvido. Obs: Esse bug j√° existe h√° algum tempo, sem um prazo claro para uma corre√ß√£o completa pelos autores. √â por isso que mostraremos um segundo m√©todo.
# 
# 2. Usar a biblioteca `BeautifulSoup` (ou similar). Embora essa abordagem seja um pouco mais manual, mostramos como ela ainda pode ser feita facilmente com apenas algumas linhas de c√≥digo.
# 
# 
# 
# #### m√©todo 1: Com par√¢metro add_video_info
# 
# Podemos passar como par√¢metro add_video_info=True (que por padr√£o √© =False) e isso far√° com que sejam retornados os metadados do v√≠deo, como: t√≠tulo, descri√ß√£o, autor, visualiza√ß√µes, e capa)
# 
# Para usar esse par√¢metro voc√™ precisa ter instalado antes a biblioteca pytube (lembre-se que instalamos anteriormente neste Colab, ent√£o se estiver executando em seu computador certifique-se que instalou essa biblioteca)
# 
# video_loader = YoutubeLoader.from_youtube_url("https://www.youtube.com/watch?v=II28i__Tf3M",
#                                                #add_video_info = True,
#                                                language = ["pt", "pt-BR", "en"],)
# 
# infos = video_loader.load()
# 
# infos
# 
# Podemos organizar desse modo
# 
# infos_video = f"""Informa√ß√µes do v√≠deo:
# 
# T√≠tulo: {infos[0].metadata['title']}
# Autor: {infos[0].metadata['author']}
# Data: {infos[0].metadata['publish_date'][:10]}
# URL: https://www.youtube.com/watch?v={infos[0].metadata['source']}
# 
# Transcri√ß√£o: {transcricao}
# """
# print(infos_video)
# 
# > **Observa√ß√£o importante:** Se aparecer esse erro para voc√™ "PytubeError: Exception while accessing title of..." tente reiniciar a sess√£o do Colab e executar novamente o mesmo c√≥digo na ordem. Essa mensagem √© referente a um bug da biblioteca usada para ler os dados do Youtube e que at√© o momento n√£o h√° um padr√£o conhecido ou uma explica√ß√£o dos autores (se esse bug for definitivamente resolvido iremos remover esse aviso). Se esse erro continuar a aparecer para voc√™, use o m√©todo 2 abaixo
# 
# #### m√©todo 2: Com BeautifulSoup
# 
# Para fazer desse modo, precisaremos importar a biblioteca `requests` e o m√©todo `bs4` do `beautifulsoup`, que j√° est√° instalado por padr√£o no Colab. Ent√£o, ao inv√©s do `infos = video_loader.load()` (conforme explicado no m√©todo 1 acima) voc√™ usar√° esse c√≥digo abaixo
# * explica√ß√£o r√°pida sobre o c√≥digo: √© feita a busca no conte√∫do HTML da p√°gina do v√≠deo no YouTube usando a fun√ß√£o `requests.get()` e, em seguida, analisa-o com o BeautifulSoup. Ele procura a tag usando `soup.find_all()`, que retorna uma lista de tags correspondentes. O t√≠tulo do v√≠deo √© extra√≠do pegando o primeiro elemento, convertendo-o em uma string e, em seguida, removendo o `<title>` e as tags com `replace()`. Ao final, o c√≥digo imprime o t√≠tulo "limpo".
# 
# import requests
# from bs4 import BeautifulSoup
# 
# def get_video_title(url):
#   r = requests.get(url)
#   soup = BeautifulSoup(r.text)
# 
#   link = soup.find_all(name="title")[0]
#   title = str(link)
#   title = title.replace("<title>","")
#   title = title.replace("</title>","")
# 
#   return title
# 
# video_url = "https://www.youtube.com/watch?v=II28i__Tf3M"
# 
# video_title = get_video_title(video_url)
# video_title
# 
# ### Reunindo as informa√ß√µes
# 
# Agora, vamos apenas combinar as informa√ß√µes do v√≠deo com a transcri√ß√£o que obtivemos anteriormente. Salvaremos tudo em uma mesma vari√°vel chamada `infos_video`
# 
# infos_video = f"""Informa√ß√µes do V√≠deo:
# 
# T√≠tulo: {video_title}
# URL: {video_url}
# 
# Transcri√ß√£o: {transcricao}
# """
# print(infos_video)
# 
# ## Salvando transcri√ß√£o em um arquivo
# 
# Esse c√≥digo abre um arquivo chamado "transcricao.txt" em modo de escrita ("w") com codifica√ß√£o UTF-8;  dentro do bloco `with` ele grava dados no arquivo.
# 
# Para cada item na vari√°vel `infos`, ele escreve o conte√∫do da vari√°vel `infos_video` no arquivo. O uso do bloco with garante que o arquivo seja fechado corretamente ap√≥s a grava√ß√£o, mesmo que ocorra algum erro durante a execu√ß√£o. E aqui n√£o precisa do `f.close()` pois o bloco with fecha o arquivo automaticamente ao finalizar
# 
# with io.open("transcricao.txt", "w", encoding="utf-8") as f:
#   for doc in infos:
#     f.write(infos_video)
# 
# ## Carregamento do modelo
# 
# Vamos reaproveitar as fun√ß√µes de carregamento que usamos nos projeos anteriores, basta copiar e colar
# 
# from langchain_huggingface import HuggingFaceEndpoint
# 
# def model_hf_hub(model = "meta-llama/Meta-Llama-3-8B-Instruct", temperature = 0.1):
#   llm = HuggingFaceEndpoint(repo_id = model,
#                        temperature = temperature,
#                        return_full_text = False,
#                        max_new_tokens = 1024,
#                        task="text-generation"
#                        )
#   return llm
# 
# def model_openai(model = "gpt-4o-mini", temperature = 0.1):
#   llm = ChatOpenAI(model = model, temperature = temperature)
#   return llm
# 
# def model_ollama(model = "phi3", temperature = 0.1):
#   llm = ChatOllama(model = model, temperature = temperature)
#   return llm
# 
# E aqui no Colab precisar setar as vari√°veis de ambiente. Pode usar o .env tamb√©m, especialmente se estiver executando localmente
# 
# os.environ["HUGGINGFACEHUB_API_TOKEN"] = getpass.getpass()
# os.environ["HF_TOKEN"] = os.environ["HUGGINGFACEHUB_API_TOKEN"]
# 
# os.environ["OPENAI_API_KEY"] = getpass.getpass()
# 
# model_class = "hf_hub" # @param ["hf_hub", "openai", "ollama"]
# 
# if model_class == "hf_hub":
#   llm = model_hf_hub()
# elif model_class == "openai":
#   llm = model_openai
# elif model_class == "ollama":
#   llm = model_ollama
# 
# 
# model_class, llm
# 
# ## Cria√ß√£o do prompt template
# 
# Vamos manter a base do prompt simples, basicamente instruindo que deve responder com base na transcri√ß√£o fornecida. Voc√™ pode modific√°-lo √† vontade depois, para deixar mais adequado ao seu objetivo ou simplesmente para tentar alcan√ßar melhores resultados
# 
# * Aqui vamos passar o transcri√ß√£o completa. Estaremos lidando com modelos que possuem uma janela grande de contexto - por exemplo o llama 3 possui algo em torno de 8k, j√° o chatGPT 4o por exemplo possui ainda mais. Deve ser uma capacidade suficiente de leitura de tokens de entrada para lidar com a maioria das transcri√ß√µes dos v√≠deos, e ser√° para todos os testados aqui.
# * Como a ideia desse projeto √© criar uma ferramenta que faz o resumo / sumariza√ß√£o ent√£o adicionar a transcri√ß√£o inteira como contexto √© at√© uma op√ß√£o mais interessante, j√° que para RAG √© recuperado geralmente uma quantidade limite de peda√ßos de documento. Portanto, se fosse usado RAG teria que configurar bem os par√¢metros, provavelmente escolher um valor maior de k por exemplo para recuperar mais documentos (no entanto, lembre-se que elevar muito esse valor aumenta o custo computacional da aplica√ß√£o)
# * Mas caso o v√≠deo seja realmente grande ent√£o pode ser interessante dividir em partes. Para isso sugerimos usar o c√≥digo do projeto 3, pode copiar as fun√ß√µes prontas que fazem as etapas de indexa√ß√£o e recupera√ß√£o (indexing & retrieval)
# 
# Al√©m da transcri√ß√£o, o prompt template ir√° aceitar a vari√°vel consulta, que nada mais √© do que a entrada para a LLM, que pode ser uma pergunta ou instru√ß√£o
# 
# E o `if model_class.startswith("hf"):` apenas copiamos do projeto anterior, lembrando que isso √© para melhorar os resultados com a implementa√ß√£o via Hugging Face Hub, que at√© o momento funciona melhor se especificarmos manualmente os tokens de in√≠cio e fim. Aqui o template √© do llama 3.x, mas for usar outro modelo open source que exija template diferente ent√£o lembre de mudar.
# 
# 
# system_prompt = "Voc√™ √© um assistente virtual prestativo e deve responder a uma consulta com base na transcri√ß√£o de um v√≠deo, que ser√° fornecida abaixo."
# 
# inputs = "Consulta: {consulta} \n Transcri√ß√£o: {transcricao}"
# 
# if model_class.startswith("hf"):
#   user_prompt = "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>".format(inputs)
# else:
#   user_prompt = "{}".format(inputs)
# 
# prompt_template = ChatPromptTemplate.from_messages([("system", system_prompt), ("user", user_prompt)])
# 
# prompt_template
# 
# ## Cria√ß√£o da chain
# 
# Nossa chain ficar√° assim
# 
# chain = prompt_template | llm | StrOutputParser()
# 
# ## Gera√ß√£o da resposta
# 
# Por fim, vamos gerar o resultado, fornecendo como par√¢metro a transcri√ß√£o e a consulta que queremos (podendo ser pergunta, instru√ß√£o, etc.)
# 
# res = chain.invoke({"transcricao": transcricao, "consulta": "resuma"})
# print(res)
# 
# Podemos melhorar esse prompt (consulta), deixando algo como `"sumarize de forma clara de entender`
# 
# res = chain.invoke({"transcricao": transcricao, "consulta": "sumarize de forma clara de entender"})
# print(res)
# 
# res = chain.invoke({"transcricao": transcricao, "consulta": "explique em 1 frase sobre o que fala esse v√≠deo"})
# print(res)
# 
# res = chain.invoke({"transcricao": transcricao, "consulta": "liste os temas desse video"})
# print(res)
# 
# ## Tradu√ß√£o da transcri√ß√£o
# 
# Para os modelos mais modernos n√£o √© necess√°rio traduzir antes, pode carregar a transcri√ß√£o no idioma desejado e passar para a LLM mesmo que no idioma diferente daquele que voc√™ escreveu as instru√ß√µes no prompt template, isso porque o modelo deve ser capaz de entender.
# 
# Mas tamb√©m √© poss√≠vel traduzir a transcri√ß√£o usando essa mesma ferramenta.
# Isso pode ser muito √∫til caso o modelo que esteja trabalhando n√£o funcione bem para m√∫ltiplos idiomas.
# 
# Para implementar isso, basta definirmos para o par√¢metro translation o c√≥digo do idioma para o qual desejamos traduzir.
# Por exemplo para o franc√™s ficaria assim
# 
# url_video = "https://www.youtube.com/watch?v=II28i__Tf3M"
# 
# video_loader = YoutubeLoader.from_youtube_url(
#     url_video,
#     add_video_info=True,
#     language=["pt", "en"],
#     translation="fr",
# )
# 
# infos = video_loader.load()
# transcricao = infos[0].page_content
# transcricao
# 
# ## Jun√ß√£o da pipeline em fun√ß√µes
# 
# Para deixar mais pr√°tico e evitar repeti√ß√µes do c√≥digo vamos reunir toda a nossa l√≥gica em fun√ß√µes, assim n√£o vai ser mais necess√°rio ficar copiando e colando o c√≥digo toda vez que for testar em outro v√≠deo
# 
# def llm_chain(model_class):
#   system_prompt = "Voc√™ √© um assistente virtual prestativo e deve responder a uma consulta com base na transcri√ß√£o de um v√≠deo, que ser√° fornecida abaixo."
# 
#   inputs = "Consulta: {consulta} \n Transcri√ß√£o: {transcricao}"
# 
#   if model_class.startswith("hf"):
#       user_prompt = "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>".format(inputs)
#   else:
#       user_prompt = "{}".format(inputs)
# 
#   prompt_template = ChatPromptTemplate.from_messages([
#       ("system", system_prompt),
#       ("user", user_prompt)
#   ])
# 
#   ### Carregamento da LLM
#   if model_class == "hf_hub":
#       llm = model_hf_hub()
#   elif model_class == "openai":
#       llm = model_openai()
#   elif model_class == "ollama":
#       llm = model_ollama()
# 
#   chain = prompt_template | llm | StrOutputParser()
# 
#   return chain
# 
# def get_video_info(url_video, language="pt", translation=None):
# 
#   video_loader = YoutubeLoader.from_youtube_url(
#       url_video,
#       #add_video_info=False,
#       language=language,
#       translation=translation,
#   )
# 
#   infos = video_loader.load()[0]
#   transcript = infos.page_content
#   video_title = get_video_title(url_video)
# 
#   return transcript, metadata
# 
# Vamos testar abaixo
# 
# transcript, metadata = get_video_info("https://www.youtube.com/watch?v=II28i__Tf3M")
# 
# metadata, transcript
# 
# Aqui aproveitamos para adicionar um Tratamento de erro com Try Catch, pois caso n√£o haja uma transcri√ß√£o para esse v√≠deo ser√° retornado um erro (possivelmente ser√° esse: `IndexError: list index out of range`), com isso n√£o ser√° poss√≠vel fazer os processamentos seguintes. Por isso programos aqui para o programa parar interromper a execu√£o se esse for caso
# 
# def interpret_video(url, query="resuma", model_class="hf_hub", language="pt", translation=None):
# 
#   try:
#     transcript, metadata = get_video_info(url, language, translation)
# 
#     chain = llm_chain(model_class)
# 
#     res = chain.invoke({"transcricao": transcript, "consulta": query})
#     print(res)
# 
#   except Exception as e:
#     print("Erro ao carregar transcri√ß√£o")
#     print(e)
# 
# ## Gera√ß√£o final
# 
# Podemos definir uma interface mais apresent√°vel no Colab atrav√©s dos comandos para deixar as vari√°veis em formato de valores de formul√°rio.
# 
# Ideias do que adicionar √† query:
# 
# * `sumarize de forma clara de entender`
# * `liste os temas desse v√≠deo`
# * `explique em 1 frase sobre o que fala esse v√≠deo`
# 
# url_video = "https://www.youtube.com/watch?v=II28i__Tf3M" # @param {type:"string"}
# query_user = "sumarize de forma clara de entender" # @param {type:"string"}
# model_class = "openai" # @param ["hf_hub", "openai", "ollama"]
# language = ["pt", "pt-BR", "en"] # @param {type:"string"}
# 
# interpret_video(url_video, query_user, model_class, language)
# 
# url_video = "https://www.youtube.com/watch?v=rEE8ERGKsqo" # @param {type:"string"}
# query_user = "sumarize de forma clara de entender" # @param {type:"string"}
# model_class = "hf_hub" # @param ["hf_hub", "openai", "ollama"]
# language = ["pt", "pt-BR", "en"] # @param {type:"string"}
# 
# interpret_video(url_video, query_user, model_class, language)
# 
# ## Explorando mais
# 
# Vamos deixar nossa aplica√ß√£o mais interessante. Podemos fazer mais de uma consulta/query de uma vez por v√≠deo.
# 
# * Para cada v√≠deo informado, podemos definir para exibir informa√ß√µes como o titulo dele;
# * e logo abaixo um resumo em um paragrafo;
# * depois uma lista de temas abordados, etc.
# 
# No Colab temos um modo interessante de fazer isso, que √© atrav√©s do Markdown.
# 
# 
# ### Exemplo com Markdown
# 
# Markdown √© um formato de linguagem de marca√ß√£o muito adotada pela comunidade por ser simples e leve, permitindo criar texto formatado com uso de s√≠mbolos especiais, como asteriscos e colchetes, em vez de tags de HTML. No Google Colab, o uso de markdown pode tornar a visualiza√ß√£o do texto mais interessante e f√°cil de ler, facilitando a compreens√£o e a apresenta√ß√£o de informa√ß√µes. Por exemplo, usando markdown, voc√™ pode deixar um texto em *it√°lico* ao deixar dentro de asteriscos, ou **negrito** se deixar ele dentro de asteriscos duplos. Tamb√©m podemos adicionar t√≠tulos com diferentes n√≠veis, por exemplo para n√≠vel 1,2,3 basta colocar antes da frase `#` `##` ou `###` respectivamente
# 
# > Mais sobre a sintaxe aqui: https://www.markdownguide.org/basic-syntax/
# 
# texto = """
# ### T√≠tulo
# descri√ß√£o em **destaque** *aqui...*
# """
# 
# from IPython.display import Markdown
# 
# display(Markdown(texto))
# 
# E abaixo em forma de lista por exemplo
# 
# Com isso, se pedirmos por exemplo pra LLM retornar uma lista ent√£o ficar√° mais apresent√°vel tamb√©m, j√° que ela ir√° retornar nesse formato com * no inicio de cada item, j√° que √© um padr√£o comum
# 
# Nos projetos em que usamos os Streamlit talvez tenha notada que j√° ficou automaticamente dessem modo, pois a interface j√° interpreta corretamente markdown
# 
# lista = """
# * item
# * item
# * item
# """
# 
# display(Markdown(lista))
# 
# ### Finaliza√ß√£o do c√≥digo
# 
# Copiamos do c√≥digo que fizemos antes, s√≥ mudando de infos[0].metadata para metadata
# 
# e ao inv√©s do print() normal, vamos colocar no lugar display(Markdown()) para deixar mais apresent√°vel
# 
# Como resposta, teremos:
# * Informa√ß√µes do v√≠deo - que s√£o os metadados: t√≠tulo, autor, data, URL.
# 
# *  Sobre o que fala o v√≠deo - resumo de 1 frase, para contextualizarmos bem rapidamente
# 
# * Temas - listagem dos principais temas desse v√≠deo
# 
# * Resposta para a consulta - que √© a resposta para a consulta personalizada que fizemos, que pode ser uma pergunta ou instru√ß√£o
# 
# Aqui voc√™ pode customizar √† vontade depois, deixar com as consultas que achar mais conveniente para o objetivo de sua aplica√ß√£o
# 
# Voc√™ poderia tamb√©m separar em duas fun√ß√µes: uma para exibir junto as consultas fixas (informa√ß√µes do v√≠deo, resumo e temas) e outra para exibir a consulta personalizada, podendo deixar em formato de chat, igual feito no projeto 1 e 2.
# 
# mas aqui estamos deixando mais simples e r√°pido, portanto ficou desse modo
# 
# def interpret_video(url, query="liste os temas desse v√≠deo", model_class="hf_hub", language="pt", translation=None):
# 
#   try:
#     transcript, video_title = get_video_info(url, language, translation)
# 
#     infos_video = f"""## Informa√ß√µes do v√≠deo
# 
#     T√≠tulo: {video_title}
#     URL: {url}
# 
#     """
# 
#     display(Markdown(infos_video))
# 
#     chain = llm_chain(model_class)
# 
#     t = "\n## Sobre o que fala o v√≠deo \n"
#     res = chain.invoke({"transcricao": transcript, "consulta": "explique em 1 frase sobre o que fala esse v√≠deo. responda direto com a frase"})
#     display(Markdown(t + res))
# 
#     t = "\n## Temas \n"
#     res = chain.invoke({"transcricao": transcript, "consulta": "lista os principais temas desse v√≠deo"})
#     display(Markdown(t + res))
# 
#     t = "\n## Resposta para a consulta \n"
#     res = chain.invoke({"transcricao": transcript, "consulta": query})
#     display(Markdown(t + res))
# 
#   except Exception as e:
#     print("Erro ao carregar transcri√ß√£o")
#     print(e)
# 
# url_video = "https://www.youtube.com/watch?v=rEE8ERGKsqo" # @param {type:"string"}
# query_user = "sumarize de forma clara de entender" # @param {type:"string"}
# model_class = "hf_hub" # @param ["hf_hub", "openai", "ollama"]
# language = ["pt", "pt-BR", "en"] # @param {type:"string"}
# 
# interpret_video(url_video, query_user, model_class, language)
# 
# url_video = "https://www.youtube.com/watch?v=n9u-TITxwoM" # @param {type:"string"}
# query_user = "sumarize de forma clara de entender" # @param {type:"string"}
# model_class = "hf_hub" # @param ["hf_hub", "openai", "ollama"]
# language = ["pt", "pt-BR", "en"] # @param {type:"string"}
# 
# interpret_video(url_video, query_user, model_class, language)
# 
# url_video = "https://www.youtube.com/watch?v=XXcHDy3QH-E" # @param {type:"string"}
# query_user = "sumarize de forma clara de entender" # @param {type:"string"}
# model_class = "hf_hub" # @param ["hf_hub", "openai", "ollama"]
# language = ["pt", "pt-BR", "en"] # @param {type:"string"}
# 
# interpret_video(url_video, query_user, model_class, language)
# 
# ## Reconhecimento de fala
# 
# Se o v√≠deo n√£o tiver uma transcri√ß√£o dispon√≠vel, ser√° necess√°rio gerar uma de forma autom√°tica utilizando um modelo de reconhecimento de fala, tamb√©m conhecido como Speech-to-Text (STT). Esses modelos convertem fala em texto a partir de arquivos de √°udio, permitindo que o conte√∫do do v√≠deo seja transcrito para ser processado pela nossa aplica√ß√£o de sumariza√ß√£o.
# 
# Um exemplo popular de modelo de Speech-to-Text √© o [Whisper](https://openai.com/index/whisper/), da OpenAI, que oferece uma solu√ß√£o robusta para transcri√ß√£o autom√°tica. Os pre√ßos e detalhes sobre o uso desse modelo podem ser encontrados na p√°gina de "[pricing](https://openai.com/api/pricing/)" da OpenAI. No entanto, neste projeto, optamos por n√£o utiliz√°-lo, pois todos os v√≠deos testados j√° possu√≠am transcri√ß√µes, muitas vezes geradas automaticamente pelo YouTube. Essa abordagem economiza processamento, j√° que o foco principal foi a implementa√ß√£o de modelos de linguagem grandes (LLMs) e a explora√ß√£o de ferramentas do Langchain.
# 
# * Se voc√™ precisar integrar um servi√ßo de transcri√ß√£o, a implementa√ß√£o √© simples, podendo ser feita com uma chamada de fun√ß√£o. A documenta√ß√£o do Langchain oferece instru√ß√µes detalhadas sobre como realizar essa integra√ß√£o aqui: https://js.langchain.com/v0.2/docs/integrations/document_loaders/file_loaders/openai_whisper_audio/.
# 
# * Na verdade voc√™ tem a liberdade de escolher qualquer servi√ßo ou c√≥digo para realizar o reconhecimento de fala. Mesmo que o servi√ßo n√£o se integre diretamente ao Langchain, isso n√£o √© um problema, pois o essencial √© obter o texto final. No final, o que voc√™ obt√©m √© um texto normal, que pode ser armazenado em um arquivo de texto ou diretamente em uma vari√°vel.
# 
# * Entretanto, se desejar aproveitar solu√ß√µes prontas dentro do Langchain, voc√™ pode utilizar o Whisper da OpenAI ou outras op√ß√µes como o [GoogleSpeechToText](https://python.langchain.com/v0.2/docs/integrations/document_loaders/google_speech_to_text/) e o [AssemblyAI Audio Transcripts](https://python.langchain.com/v0.2/docs/integrations/document_loaders/assemblyai/), que tamb√©m s√£o altamente eficazes e f√°ceis de integrar ao langchain.
# 
# ## Indo al√©m üöÄ
# 
# 1. Conforme mencionado anteriormente, para v√≠deos muito longos ou transcri√ß√µes maiores, pode ser necess√°rio dividir a transcri√ß√£o em partes menores e aplicar t√©cnicas de indexa√ß√£o e recupera√ß√£o utilizando RAG.
#  * Isso √© essencial caso o v√≠deo que deseja processar possua horas por exemplo - talvez modelos muito modernos e com janela de contexto maior consigam lidar, mas mesmo que consigam pode ser interessante aplicar essas t√©cnicas, principalmente caso o que busque n√£o seja a sumariza√ß√£o mas sim algo pr√≥ximo do nosso projeto de perguntas e respostas com documentos, que ser√° visto no projeto 3.
#  * Caso deseje seguir esse caminho, basta reutilizar o c√≥digo j√° pronto no projeto 3, que lida com esse tipo de segmenta√ß√£o e recupera√ß√£o de forma eficiente. Assim, voc√™ poder√° manter a integridade da transcri√ß√£o, otimizando o processamento e a gera√ß√£o de respostas mais precisas, mesmo com grandes volumes de dados.
# 
# 2. Como ideia adicional ou desafio: √© poss√≠vel personalizar este projeto para atender a outras necessidades, ou ainda usar a sumariza√ß√£o como uma etapa posterior dentro de uma pipeline maior.
#   * Um exemplo seria o uso da ferramenta [YouTubeSearchTool](https://python.langchain.com/v0.2/docs/integrations/tools/youtube//) do Langchain, que permite buscar automaticamente uma lista de URLs de v√≠deos no YouTube com base em um tema ou palavra-chave fornecida pelo usu√°rio.
#   * Nessa abordagem, voc√™ poderia implementar uma aplica√ß√£o que solicita um termo de busca e, em seguida, utiliza essa ferramenta para buscar v√≠deos relevantes, retornando em uma lista. Por fim, basta criar um la√ßo de repeti√ß√£o que execute a fun√ß√£o interpret_video (que criamos nesse projeto) para cada v√≠deo da lista, realizando assim a sumariza√ß√£o de m√∫ltiplos v√≠deos associados a um tema de forma automatizada.
# 
# 3. Al√©m disso, √© poss√≠vel integrar essa funcionalidade a uma interface interativa com ferramentas como Streamlit, que permite criar facilmente interfaces gr√°ficas.
#  * Os m√©todos para essa integra√ß√£o est√£o descritos detalhadamente nos projetos 2 e 3, caso queira expandir a aplica√ß√£o.
#  * Embora, neste exemplo, tenhamos optado por usar o Google Colab pela conveni√™ncia e para demonstrar a exibi√ß√£o com Markdown, a migra√ß√£o para uma interface mais robusta pode ser feita com facilidade e oferecer uma experi√™ncia mais fluida para o usu√°rio final caso deseje publicar essa aplica√ß√£o.
# 
