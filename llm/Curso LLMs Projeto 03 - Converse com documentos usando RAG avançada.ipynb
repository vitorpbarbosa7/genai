{"cells":[{"cell_type":"markdown","metadata":{"id":"CInd-FXyVoZC"},"source":["# Projeto 03 - Converse com documentos usando RAG avançada\n","\n","> Nesse projeto iremos aprender a como criar uma pipeline de RAG mais avançada para que seja capaz de:\n","* fazer perguntas a algum documento lido, como se fosse um chat com o próprio arquivo.\n","* consultar mais de uma referência ao mesmo tempo.\n","* entender o contexto das mensagens passadas, usando o histórico da conversa também como uma referência para formular a resposta\n","\n","E para essa aplicação também será construída uma interface.\n","Portanto, podemos reaproveitar parte do código do projeto anterior e assim ir adicionando as novas funcionalidades"]},{"cell_type":"markdown","metadata":{"id":"2Khq09Wegw2H"},"source":["## [ ! ] Como executar em ambiente local\n","Para executar o código desse projeto em um ambiente local, siga as instruções para instalar as dependências necessárias usando os comandos abaixo. Você pode usar os mesmos comandos de instalação.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EqT4YzGtWZGq"},"source":["## Instalação e Configuração\n","\n","Aqui iremos carregar primeiramente todos as funções que usamos no projeto anterior e mais algumas outras. Entra elas, o FAISS (um vectorstore no mesmo estilo do Chroma, que usamos nas aulas anteriores sobre RAG) e também outras funções necessárias para implementação de uma pipeline RAG que entenda o contexto das conversas.\n","\n","Lembrando: podemos reaproveitar parte do código que criamos no projeto 02.\n","Então se quiser pode fazer uma cópia e fazer as modificações a partir dele.\n","\n","Abaixo, cada mudança que será feita a partir desse arquivo, assim poderá ir acompanhando as alterações"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OEG8KS746Wmu","outputId":"3484eda1-56a3-4957-ac4f-f2b6c2d983a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m722.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q streamlit langchain\n","!pip install -q langchain_community langchain-huggingface langchain_ollama langchain_openai"]},{"cell_type":"markdown","metadata":{"id":"tv4u-p4J0l9K"},"source":["### Instalação do FAISS\n","\n","Antes de importar é necessário que instalemos o FAISS, ele não vem instalado por padrão no Colab. Portanto, podemos usar aqui e em ambiente local esse mesmo comando para instalar:\n","\n","`pip install -q faiss-cpu`\n","\n","você também pode instalar `faiss-gpu` caso queira usar a versão otimizada para GPU. Para simplificar no momento, iremos usar a versão padrão por CPU mesmo\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-hJoWdJEBmU"},"outputs":[],"source":["!pip install -q faiss-cpu"]},{"cell_type":"markdown","metadata":{"id":"paEkaqQI0o7V"},"source":["Em seguida, use em sua aplicação o `import faiss` e também importar o `FAISS` dentro de biblioteca langchain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MP19iHfeEETw"},"outputs":[],"source":["import faiss\n","from langchain_community.vectorstores import FAISS"]},{"cell_type":"markdown","metadata":{"id":"ULIGaJSU0qca"},"source":["### Instalação do PyPDFLoader\n","\n","Usaremos o PyPDFLoader para fazer a leitura dos arquivos PDF em nossa aplicação. Isso será explicado com detalhes na devida seção.\n","E para podermos usá-lo, precisamos antes instalar a biblioteca com o comando abaixo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jf2iNG4hEGo-"},"outputs":[],"source":["!pip install pypdf"]},{"cell_type":"markdown","metadata":{"id":"yUTV--pa0tHW"},"source":["### Demais instalações\n","\n","Assim como no projeto anterior, vamos instalar aqui o dotenv de novo (em ambiente local não precisa executar a instalação de novo, mas aqui no Colab como é uma nova sessão precisamos) e também o localtunnel (lembrando que esse não é necessário em ambiente local)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xo38vjjlEJb6"},"outputs":[],"source":["!pip install -q python-dotenv\n","!npm install -q localtunnel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1734211864634,"user":{"displayName":"Gabriel Alves","userId":"07503873055563742982"},"user_tz":180},"id":"uHxWcGDqEQZ0","outputId":"5670b28e-5566-4280-9cf3-e859af1d894e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing .env\n"]}],"source":["%%writefile .env\n","HUGGINGFACE_API_KEY=##########\n","HUGGINGFACEHUB_API_TOKEN=##########\n","OPENAI_API_KEY=##########\n","TAVILY_API_KEY=##########\n","SERPAPI_API_KEY=##########\n","LANGCHAIN_API_KEY=##########"]},{"cell_type":"markdown","metadata":{"id":"gmGl3IJyUB0U"},"source":["---\n","\n","## Inicialização da interface\n","\n","Por fim, reunimos todo o código em um único script e adicionamos a configuração da página com st.set_page_config e st.title, alterando o título e o emoji para deixar a interface mais personalizada e adequada ao projeto atual, com um visual mais alinhado com o contexto desse projeto"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1734211864635,"user":{"displayName":"Gabriel Alves","userId":"07503873055563742982"},"user_tz":180},"id":"0P5CTavqvh74","outputId":"73013970-0efc-438c-f5f9-9c68b7219e07"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing projeto3.py\n"]}],"source":["%%writefile projeto3.py\n","\n","import streamlit as st\n","from langchain_core.messages import AIMessage, HumanMessage\n","from langchain_core.prompts import MessagesPlaceholder\n","\n","from langchain_ollama import ChatOllama\n","from langchain_openai import ChatOpenAI\n","\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n","\n","import torch\n","from langchain_huggingface import ChatHuggingFace\n","from langchain_community.llms import HuggingFaceHub\n","\n","import faiss\n","import tempfile\n","import os\n","import time\n","from langchain_community.vectorstores import FAISS\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","# Configurações do Streamlit\n","st.set_page_config(page_title=\"Converse com documentos 📚\", page_icon=\"📚\")\n","st.title(\"Converse com documentos 📚\")\n","\n","model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n","\n","## Provedores de modelos\n","def model_hf_hub(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", temperature=0.1):\n","  llm = HuggingFaceHub(\n","      repo_id=model,\n","      model_kwargs={\n","          \"temperature\": temperature,\n","          \"return_full_text\": False,\n","          \"max_new_tokens\": 512,\n","          #\"stop\": [\"<|eot_id|>\"],\n","          # demais parâmetros que desejar\n","      }\n","  )\n","  return llm\n","\n","def model_openai(model=\"gpt-4o-mini\", temperature=0.1):\n","    llm = ChatOpenAI(\n","        model=model,\n","        temperature=temperature\n","        # demais parâmetros que desejar\n","    )\n","    return llm\n","\n","def model_ollama(model=\"phi3\", temperature=0.1):\n","    llm = ChatOllama(\n","        model=model,\n","        temperature=temperature,\n","    )\n","    return llm\n","\n","\n","## Indexação e Recuperação\n","\n","def config_retriever(uploads):\n","    # Carregar documentos\n","    docs = []\n","    temp_dir = tempfile.TemporaryDirectory()\n","    for file in uploads:\n","        temp_filepath = os.path.join(temp_dir.name, file.name)\n","        with open(temp_filepath, \"wb\") as f:\n","            f.write(file.getvalue())\n","        loader = PyPDFLoader(temp_filepath)\n","        docs.extend(loader.load())\n","\n","    # Divisão em pedaços de texto / Split\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=1000,\n","        chunk_overlap=200\n","    )\n","    splits = text_splitter.split_documents(docs)\n","\n","    # Embeddings\n","    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n","\n","    # Armazenamento\n","    vectorstore = FAISS.from_documents(splits, embeddings)\n","\n","    vectorstore.save_local('vectorstore/db_faiss')\n","\n","    # Configurando o recuperador de texto / Retriever\n","    retriever = vectorstore.as_retriever(\n","        search_type='mmr',\n","        search_kwargs={'k':3, 'fetch_k':4}\n","    )\n","\n","    return retriever\n","\n","\n","def config_rag_chain(model_class, retriever):\n","\n","    ### Carregamento da LLM\n","    if model_class == \"hf_hub\":\n","        llm = model_hf_hub()\n","    elif model_class == \"openai\":\n","        llm = model_openai()\n","    elif model_class == \"ollama\":\n","        llm = model_ollama()\n","\n","    # Para definição dos prompts\n","    if model_class.startswith(\"hf\"):\n","        token_s, token_e = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\", \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n","    else:\n","        token_s, token_e = \"\", \"\"\n","\n","    # Prompt de contextualização\n","    context_q_system_prompt = \"Given the following chat history and the follow-up question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n","\n","    context_q_system_prompt = token_s + context_q_system_prompt\n","    context_q_user_prompt = \"Question: {input}\" + token_e\n","    context_q_prompt = ChatPromptTemplate.from_messages(\n","        [\n","            (\"system\", context_q_system_prompt),\n","            MessagesPlaceholder(\"chat_history\"),\n","            (\"human\", context_q_user_prompt),\n","        ]\n","    )\n","\n","    # Chain para contextualização\n","    history_aware_retriever = create_history_aware_retriever(\n","        llm=llm, retriever=retriever, prompt=context_q_prompt\n","    )\n","\n","    # Prompt para perguntas e respostas (Q&A)\n","    qa_prompt_template = \"\"\"Você é um assistente virtual prestativo e está respondendo perguntas gerais.\n","    Use os seguintes pedaços de contexto recuperado para responder à pergunta.\n","    Se você não sabe a resposta, apenas diga que não sabe. Mantenha a resposta concisa.\n","    Responda em português. \\n\\n\n","    Pergunta: {input} \\n\n","    Contexto: {context}\"\"\"\n","\n","    qa_prompt = PromptTemplate.from_template(token_s + qa_prompt_template + token_e)\n","\n","    # Configurar LLM e Chain para perguntas e respostas (Q&A)\n","\n","    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n","\n","    rag_chain = create_retrieval_chain(\n","        history_aware_retriever,\n","        qa_chain,\n","    )\n","\n","    return rag_chain\n","\n","\n","## Cria painel lateral na interface\n","uploads = st.sidebar.file_uploader(\n","    label=\"Enviar arquivos\", type=[\"pdf\"],\n","    accept_multiple_files=True\n",")\n","if not uploads:\n","    st.info(\"Por favor, envie algum arquivo para continuar!\")\n","    st.stop()\n","\n","\n","if \"chat_history\" not in st.session_state:\n","    st.session_state.chat_history = [\n","        AIMessage(content=\"Olá, sou o seu assistente virtual! Como posso ajudar você?\"),\n","    ]\n","\n","if \"docs_list\" not in st.session_state:\n","    st.session_state.docs_list = None\n","\n","if \"retriever\" not in st.session_state:\n","    st.session_state.retriever = None\n","\n","for message in st.session_state.chat_history:\n","    if isinstance(message, AIMessage):\n","        with st.chat_message(\"AI\"):\n","            st.write(message.content)\n","    elif isinstance(message, HumanMessage):\n","        with st.chat_message(\"Human\"):\n","            st.write(message.content)\n","\n","# para gravar quanto tempo levou para a geração\n","start = time.time()\n","user_query = st.chat_input(\"Digite sua mensagem aqui...\")\n","\n","if user_query is not None and user_query != \"\" and uploads is not None:\n","\n","    st.session_state.chat_history.append(HumanMessage(content=user_query))\n","\n","    with st.chat_message(\"Human\"):\n","        st.markdown(user_query)\n","\n","    with st.chat_message(\"AI\"):\n","\n","        if st.session_state.docs_list != uploads:\n","            print(uploads)\n","            st.session_state.docs_list = uploads\n","            st.session_state.retriever = config_retriever(uploads)\n","\n","        rag_chain = config_rag_chain(model_class, st.session_state.retriever)\n","\n","        result = rag_chain.invoke({\"input\": user_query, \"chat_history\": st.session_state.chat_history})\n","\n","        resp = result['answer']\n","        st.write(resp)\n","\n","        # mostrar a fonte\n","        sources = result['context']\n","        for idx, doc in enumerate(sources):\n","            source = doc.metadata['source']\n","            file = os.path.basename(source)\n","            page = doc.metadata.get('page', 'Página não especificada')\n","\n","            ref = f\":link: Fonte {idx}: *{file} - p. {page}*\"\n","            print(ref)\n","            with st.popover(ref):\n","                st.caption(doc.page_content)\n","\n","    st.session_state.chat_history.append(AIMessage(content=resp))\n","\n","end = time.time()\n","print(\"Tempo: \", end - start)"]},{"cell_type":"markdown","metadata":{"id":"M54-PNNEPrxh"},"source":["### Execução do Streamlit\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KrnuMrbds4Na"},"outputs":[],"source":["!streamlit run projeto3.py &>/content/logs.txt &"]},{"cell_type":"markdown","metadata":{"id":"yYiheBeuN8SA"},"source":["E agora para nos conectar, usamos o comando abaixo (explicações no colab do projeto 02)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"IMjgsW8c1NvZ","outputId":"e17beb7c-83fe-400c-b102-5e10b36a5602"},"outputs":[{"name":"stdout","output_type":"stream","text":["34.16.159.1\n","\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://short-lizards-hunt.loca.lt\n"]}],"source":["!wget -q -O - ipv4.icanhazip.com\n","\n","!npx localtunnel --port 8501"]},{"cell_type":"markdown","metadata":{"id":"Ex1IJrK34UCw"},"source":["## Como melhorar 🚀\n","\n","Sabendo exatamente como cada função opera e entendendo as explicações tratadas ao longo deste projeto, você tem o conhecimento necessário para melhorar os resultados da sua aplicação RAG. Vamos listar abaixo as estratégias que podem ser aplicadas para otimizar a qualidade das respostas e a eficiência do sistema:\n","\n","* Testar outros modelos de Embedding - conforme citado, a seleção do modelo correto para o sistema RAG é crucial, pois afeta diretamente a precisão das respostas, além da utilização de recursos e a escalabilidade da aplicação. Escolher modelos que funcionem bem com o idioma e a tarefa em questão pode melhorar significativamente os resultados. Ao testar diferentes modelos, você pode identificar qual oferece a melhor combinação de qualidade e eficiência para as suas necessidades.\n","\n","* Ajustar o prompt fixo (do sistema) - Modificar o prompt do sistema para torná-lo mais explícito sobre as funções que a LLM deve desempenhar pode melhorar os resultados. O prompt deve especificar com clareza o que a LLM deve priorizar na resposta e o que deve ser ignorado. Isso orienta o modelo a focar no que é mais relevante para sua aplicação e seu objetivo.\n","\n","* Melhorar o prompt do usuário - lembrar o usuário (colocando um aviso na interface talvez) que quanto mais específico for na pergunta maior a chance de aumentar a precisão das respostas geradas pela LLM. Quanto mais detalhado e claro o pedido, mais relevante será o retorno. Esta prática também ajuda a reduzir ambiguidades que podem prejudicar a interpretação da consulta pelo modelo.\n","\n","* Ajustar o prompt de contextualização - lembrando que este prompt reformula a pergunta do usuário com base no histórico da conversa, algo útil quando a consulta precisa de contexto para ser corretamente interpretada. O prompt de contextualização (context_q_system_prompt) instrui o modelo a levar o histórico em consideração; e embora o prompt atual esteja em inglês devido à maior chance de compatibilidade da LLM com este idioma (apesar de ser compatível com o nosso), você pode testá-lo em português e assim fica fácil modificar o texto para maximizar o desempenho no idioma desejado.\n","\n","* Testar outras LLMs - Explorar outros modelos de linguagem, especialmente aqueles que aceitam uma quantidade maior de tokens e têm bom desempenho no idioma escolhido, pode melhorar a performance. Para casos mais exigentes, pode valer a pena considerar soluções proprietárias como o ChatGPT ou serviços pagos (como o Groq, citado no Colab 1) que disponibilizam grandes modelos de código aberto. Modelos maiores podem lidar melhor com consultas complexas e fornecer respostas mais elaboradas.\n","\n","* Ajustar os parâmetros de recuperação (k e fetch_k) - Modificar os parâmetros das etapas de recuperação, como os valores de k e fetch_k, pode ter um impacto significativo no desempenho da sua aplicação. Experimente começar com valores menores e aumentá-los conforme necessário, sempre monitorando o impacto na relevância e qualidade das respostas. Para mais detalhes, consulte a seção da pipeline RAG e o retriever. Outra ideia seria testar outros algoritmos além do MMR.\n","\n","* Deixar melhor preparado para aceitar qualquer documento - uma ideia é fazer o preprocessamento de arquivos PDF (ou outros formatos) para adequação ao vector store. Muitas vezes PDFs possuem tabelas ou outras estruturas que dificultam a interpretação; ou ainda, documentos em formatos mais diferentes como HTML, CSV, ou PPTX não estão estruturados para extração ideal de informações. A preparação desses arquivos é crucial para garantir que o conteúdo relevante seja corretamente capturado e disponibilizado para o sistema de recuperação.\n"," * Existem soluções especializadas automatizam essa transformação, organizando os dados e eliminando informações desnecessárias. Isso otimiza o fluxo de trabalho e melhora a precisão dos resultados. Um exemplo é o serviço Unstructured (Acesse https://unstructured.io), que facilita a extração de dados complexos de arquivos, tornando-os prontos para uso em bancos de dados vetoriais e frameworks de LLMs, o que aumenta a qualidade da recuperação da informação e o desempenho da aplicação RAG.\n"," * Para usar isso no langchain é simples, você pode usar o método de Document Loader. Na prática, basta carregar o documento usando o document loader Unstructured (ao invés do PyPDFLoader que usamos). Mais detalhes aqui: https://python.langchain.com/docs/integrations/document_loaders/unstructured_file/\n","\n","\n","Essas estratégias visam otimizar a eficiência e a qualidade das respostas do sistema RAG, adaptando-o ao seu caso de uso específico.\n","\n","\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1ffUdoTetTjHOx3HIGi-0nvvg31e9cVYU","timestamp":1724761843583}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}