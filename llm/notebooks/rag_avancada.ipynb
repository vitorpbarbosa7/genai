{"cells":[{"cell_type":"markdown","metadata":{"id":"CInd-FXyVoZC"},"source":["# Projeto 03 - Converse com documentos usando RAG avanÃ§ada\n","\n","> Nesse projeto iremos aprender a como criar uma pipeline de RAG mais avanÃ§ada para que seja capaz de:\n","* fazer perguntas a algum documento lido, como se fosse um chat com o prÃ³prio arquivo.\n","* consultar mais de uma referÃªncia ao mesmo tempo.\n","* entender o contexto das mensagens passadas, usando o histÃ³rico da conversa tambÃ©m como uma referÃªncia para formular a resposta\n","\n","E para essa aplicaÃ§Ã£o tambÃ©m serÃ¡ construÃ­da uma interface.\n","Portanto, podemos reaproveitar parte do cÃ³digo do projeto anterior e assim ir adicionando as novas funcionalidades"]},{"cell_type":"markdown","metadata":{"id":"2Khq09Wegw2H"},"source":["## [ ! ] Como executar em ambiente local\n","Para executar o cÃ³digo desse projeto em um ambiente local, siga as instruÃ§Ãµes para instalar as dependÃªncias necessÃ¡rias usando os comandos abaixo. VocÃª pode usar os mesmos comandos de instalaÃ§Ã£o.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EqT4YzGtWZGq"},"source":["## InstalaÃ§Ã£o e ConfiguraÃ§Ã£o\n","\n","Aqui iremos carregar primeiramente todos as funÃ§Ãµes que usamos no projeto anterior e mais algumas outras. Entra elas, o FAISS (um vectorstore no mesmo estilo do Chroma, que usamos nas aulas anteriores sobre RAG) e tambÃ©m outras funÃ§Ãµes necessÃ¡rias para implementaÃ§Ã£o de uma pipeline RAG que entenda o contexto das conversas.\n","\n","Lembrando: podemos reaproveitar parte do cÃ³digo que criamos no projeto 02.\n","EntÃ£o se quiser pode fazer uma cÃ³pia e fazer as modificaÃ§Ãµes a partir dele.\n","\n","Abaixo, cada mudanÃ§a que serÃ¡ feita a partir desse arquivo, assim poderÃ¡ ir acompanhando as alteraÃ§Ãµes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OEG8KS746Wmu","outputId":"3484eda1-56a3-4957-ac4f-f2b6c2d983a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m722.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q streamlit langchain\n","!pip install -q langchain_community langchain-huggingface langchain_ollama langchain_openai"]},{"cell_type":"markdown","metadata":{"id":"tv4u-p4J0l9K"},"source":["### InstalaÃ§Ã£o do FAISS\n","\n","Antes de importar Ã© necessÃ¡rio que instalemos o FAISS, ele nÃ£o vem instalado por padrÃ£o no Colab. Portanto, podemos usar aqui e em ambiente local esse mesmo comando para instalar:\n","\n","`pip install -q faiss-cpu`\n","\n","vocÃª tambÃ©m pode instalar `faiss-gpu` caso queira usar a versÃ£o otimizada para GPU. Para simplificar no momento, iremos usar a versÃ£o padrÃ£o por CPU mesmo\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-hJoWdJEBmU"},"outputs":[],"source":["!pip install -q faiss-cpu"]},{"cell_type":"markdown","metadata":{"id":"paEkaqQI0o7V"},"source":["Em seguida, use em sua aplicaÃ§Ã£o o `import faiss` e tambÃ©m importar o `FAISS` dentro de biblioteca langchain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MP19iHfeEETw"},"outputs":[],"source":["import faiss\n","from langchain_community.vectorstores import FAISS"]},{"cell_type":"markdown","metadata":{"id":"ULIGaJSU0qca"},"source":["### InstalaÃ§Ã£o do PyPDFLoader\n","\n","Usaremos o PyPDFLoader para fazer a leitura dos arquivos PDF em nossa aplicaÃ§Ã£o. Isso serÃ¡ explicado com detalhes na devida seÃ§Ã£o.\n","E para podermos usÃ¡-lo, precisamos antes instalar a biblioteca com o comando abaixo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jf2iNG4hEGo-"},"outputs":[],"source":["!pip install pypdf"]},{"cell_type":"markdown","metadata":{"id":"yUTV--pa0tHW"},"source":["### Demais instalaÃ§Ãµes\n","\n","Assim como no projeto anterior, vamos instalar aqui o dotenv de novo (em ambiente local nÃ£o precisa executar a instalaÃ§Ã£o de novo, mas aqui no Colab como Ã© uma nova sessÃ£o precisamos) e tambÃ©m o localtunnel (lembrando que esse nÃ£o Ã© necessÃ¡rio em ambiente local)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xo38vjjlEJb6"},"outputs":[],"source":["!pip install -q python-dotenv\n","!npm install -q localtunnel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1734211864634,"user":{"displayName":"Gabriel Alves","userId":"07503873055563742982"},"user_tz":180},"id":"uHxWcGDqEQZ0","outputId":"5670b28e-5566-4280-9cf3-e859af1d894e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing .env\n"]}],"source":["%%writefile .env\n","HUGGINGFACE_API_KEY=##########\n","HUGGINGFACEHUB_API_TOKEN=##########\n","OPENAI_API_KEY=##########\n","TAVILY_API_KEY=##########\n","SERPAPI_API_KEY=##########\n","LANGCHAIN_API_KEY=##########"]},{"cell_type":"markdown","metadata":{"id":"gmGl3IJyUB0U"},"source":["---\n","\n","## InicializaÃ§Ã£o da interface\n","\n","Por fim, reunimos todo o cÃ³digo em um Ãºnico script e adicionamos a configuraÃ§Ã£o da pÃ¡gina com st.set_page_config e st.title, alterando o tÃ­tulo e o emoji para deixar a interface mais personalizada e adequada ao projeto atual, com um visual mais alinhado com o contexto desse projeto"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1734211864635,"user":{"displayName":"Gabriel Alves","userId":"07503873055563742982"},"user_tz":180},"id":"0P5CTavqvh74","outputId":"73013970-0efc-438c-f5f9-9c68b7219e07"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing projeto3.py\n"]}],"source":["%%writefile projeto3.py\n","\n","import streamlit as st\n","from langchain_core.messages import AIMessage, HumanMessage\n","from langchain_core.prompts import MessagesPlaceholder\n","\n","from langchain_ollama import ChatOllama\n","from langchain_openai import ChatOpenAI\n","\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n","\n","import torch\n","from langchain_huggingface import ChatHuggingFace\n","from langchain_community.llms import HuggingFaceHub\n","\n","import faiss\n","import tempfile\n","import os\n","import time\n","from langchain_community.vectorstores import FAISS\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","# ConfiguraÃ§Ãµes do Streamlit\n","st.set_page_config(page_title=\"Converse com documentos ğŸ“š\", page_icon=\"ğŸ“š\")\n","st.title(\"Converse com documentos ğŸ“š\")\n","\n","model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n","\n","## Provedores de modelos\n","def model_hf_hub(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", temperature=0.1):\n","  llm = HuggingFaceHub(\n","      repo_id=model,\n","      model_kwargs={\n","          \"temperature\": temperature,\n","          \"return_full_text\": False,\n","          \"max_new_tokens\": 512,\n","          #\"stop\": [\"<|eot_id|>\"],\n","          # demais parÃ¢metros que desejar\n","      }\n","  )\n","  return llm\n","\n","def model_openai(model=\"gpt-4o-mini\", temperature=0.1):\n","    llm = ChatOpenAI(\n","        model=model,\n","        temperature=temperature\n","        # demais parÃ¢metros que desejar\n","    )\n","    return llm\n","\n","def model_ollama(model=\"phi3\", temperature=0.1):\n","    llm = ChatOllama(\n","        model=model,\n","        temperature=temperature,\n","    )\n","    return llm\n","\n","\n","## IndexaÃ§Ã£o e RecuperaÃ§Ã£o\n","\n","def config_retriever(uploads):\n","    # Carregar documentos\n","    docs = []\n","    temp_dir = tempfile.TemporaryDirectory()\n","    for file in uploads:\n","        temp_filepath = os.path.join(temp_dir.name, file.name)\n","        with open(temp_filepath, \"wb\") as f:\n","            f.write(file.getvalue())\n","        loader = PyPDFLoader(temp_filepath)\n","        docs.extend(loader.load())\n","\n","    # DivisÃ£o em pedaÃ§os de texto / Split\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=1000,\n","        chunk_overlap=200\n","    )\n","    splits = text_splitter.split_documents(docs)\n","\n","    # Embeddings\n","    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n","\n","    # Armazenamento\n","    vectorstore = FAISS.from_documents(splits, embeddings)\n","\n","    vectorstore.save_local('vectorstore/db_faiss')\n","\n","    # Configurando o recuperador de texto / Retriever\n","    retriever = vectorstore.as_retriever(\n","        search_type='mmr',\n","        search_kwargs={'k':3, 'fetch_k':4}\n","    )\n","\n","    return retriever\n","\n","\n","def config_rag_chain(model_class, retriever):\n","\n","    ### Carregamento da LLM\n","    if model_class == \"hf_hub\":\n","        llm = model_hf_hub()\n","    elif model_class == \"openai\":\n","        llm = model_openai()\n","    elif model_class == \"ollama\":\n","        llm = model_ollama()\n","\n","    # Para definiÃ§Ã£o dos prompts\n","    if model_class.startswith(\"hf\"):\n","        token_s, token_e = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\", \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n","    else:\n","        token_s, token_e = \"\", \"\"\n","\n","    # Prompt de contextualizaÃ§Ã£o\n","    context_q_system_prompt = \"Given the following chat history and the follow-up question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n","\n","    context_q_system_prompt = token_s + context_q_system_prompt\n","    context_q_user_prompt = \"Question: {input}\" + token_e\n","    context_q_prompt = ChatPromptTemplate.from_messages(\n","        [\n","            (\"system\", context_q_system_prompt),\n","            MessagesPlaceholder(\"chat_history\"),\n","            (\"human\", context_q_user_prompt),\n","        ]\n","    )\n","\n","    # Chain para contextualizaÃ§Ã£o\n","    history_aware_retriever = create_history_aware_retriever(\n","        llm=llm, retriever=retriever, prompt=context_q_prompt\n","    )\n","\n","    # Prompt para perguntas e respostas (Q&A)\n","    qa_prompt_template = \"\"\"VocÃª Ã© um assistente virtual prestativo e estÃ¡ respondendo perguntas gerais.\n","    Use os seguintes pedaÃ§os de contexto recuperado para responder Ã  pergunta.\n","    Se vocÃª nÃ£o sabe a resposta, apenas diga que nÃ£o sabe. Mantenha a resposta concisa.\n","    Responda em portuguÃªs. \\n\\n\n","    Pergunta: {input} \\n\n","    Contexto: {context}\"\"\"\n","\n","    qa_prompt = PromptTemplate.from_template(token_s + qa_prompt_template + token_e)\n","\n","    # Configurar LLM e Chain para perguntas e respostas (Q&A)\n","\n","    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n","\n","    rag_chain = create_retrieval_chain(\n","        history_aware_retriever,\n","        qa_chain,\n","    )\n","\n","    return rag_chain\n","\n","\n","## Cria painel lateral na interface\n","uploads = st.sidebar.file_uploader(\n","    label=\"Enviar arquivos\", type=[\"pdf\"],\n","    accept_multiple_files=True\n",")\n","if not uploads:\n","    st.info(\"Por favor, envie algum arquivo para continuar!\")\n","    st.stop()\n","\n","\n","if \"chat_history\" not in st.session_state:\n","    st.session_state.chat_history = [\n","        AIMessage(content=\"OlÃ¡, sou o seu assistente virtual! Como posso ajudar vocÃª?\"),\n","    ]\n","\n","if \"docs_list\" not in st.session_state:\n","    st.session_state.docs_list = None\n","\n","if \"retriever\" not in st.session_state:\n","    st.session_state.retriever = None\n","\n","for message in st.session_state.chat_history:\n","    if isinstance(message, AIMessage):\n","        with st.chat_message(\"AI\"):\n","            st.write(message.content)\n","    elif isinstance(message, HumanMessage):\n","        with st.chat_message(\"Human\"):\n","            st.write(message.content)\n","\n","# para gravar quanto tempo levou para a geraÃ§Ã£o\n","start = time.time()\n","user_query = st.chat_input(\"Digite sua mensagem aqui...\")\n","\n","if user_query is not None and user_query != \"\" and uploads is not None:\n","\n","    st.session_state.chat_history.append(HumanMessage(content=user_query))\n","\n","    with st.chat_message(\"Human\"):\n","        st.markdown(user_query)\n","\n","    with st.chat_message(\"AI\"):\n","\n","        if st.session_state.docs_list != uploads:\n","            print(uploads)\n","            st.session_state.docs_list = uploads\n","            st.session_state.retriever = config_retriever(uploads)\n","\n","        rag_chain = config_rag_chain(model_class, st.session_state.retriever)\n","\n","        result = rag_chain.invoke({\"input\": user_query, \"chat_history\": st.session_state.chat_history})\n","\n","        resp = result['answer']\n","        st.write(resp)\n","\n","        # mostrar a fonte\n","        sources = result['context']\n","        for idx, doc in enumerate(sources):\n","            source = doc.metadata['source']\n","            file = os.path.basename(source)\n","            page = doc.metadata.get('page', 'PÃ¡gina nÃ£o especificada')\n","\n","            ref = f\":link: Fonte {idx}: *{file} - p. {page}*\"\n","            print(ref)\n","            with st.popover(ref):\n","                st.caption(doc.page_content)\n","\n","    st.session_state.chat_history.append(AIMessage(content=resp))\n","\n","end = time.time()\n","print(\"Tempo: \", end - start)"]},{"cell_type":"markdown","metadata":{"id":"M54-PNNEPrxh"},"source":["### ExecuÃ§Ã£o do Streamlit\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KrnuMrbds4Na"},"outputs":[],"source":["!streamlit run projeto3.py &>/content/logs.txt &"]},{"cell_type":"markdown","metadata":{"id":"yYiheBeuN8SA"},"source":["E agora para nos conectar, usamos o comando abaixo (explicaÃ§Ãµes no colab do projeto 02)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"IMjgsW8c1NvZ","outputId":"e17beb7c-83fe-400c-b102-5e10b36a5602"},"outputs":[{"name":"stdout","output_type":"stream","text":["34.16.159.1\n","\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kyour url is: https://short-lizards-hunt.loca.lt\n"]}],"source":["!wget -q -O - ipv4.icanhazip.com\n","\n","!npx localtunnel --port 8501"]},{"cell_type":"markdown","metadata":{"id":"Ex1IJrK34UCw"},"source":["## Como melhorar ğŸš€\n","\n","Sabendo exatamente como cada funÃ§Ã£o opera e entendendo as explicaÃ§Ãµes tratadas ao longo deste projeto, vocÃª tem o conhecimento necessÃ¡rio para melhorar os resultados da sua aplicaÃ§Ã£o RAG. Vamos listar abaixo as estratÃ©gias que podem ser aplicadas para otimizar a qualidade das respostas e a eficiÃªncia do sistema:\n","\n","* Testar outros modelos de Embedding - conforme citado, a seleÃ§Ã£o do modelo correto para o sistema RAG Ã© crucial, pois afeta diretamente a precisÃ£o das respostas, alÃ©m da utilizaÃ§Ã£o de recursos e a escalabilidade da aplicaÃ§Ã£o. Escolher modelos que funcionem bem com o idioma e a tarefa em questÃ£o pode melhorar significativamente os resultados. Ao testar diferentes modelos, vocÃª pode identificar qual oferece a melhor combinaÃ§Ã£o de qualidade e eficiÃªncia para as suas necessidades.\n","\n","* Ajustar o prompt fixo (do sistema) - Modificar o prompt do sistema para tornÃ¡-lo mais explÃ­cito sobre as funÃ§Ãµes que a LLM deve desempenhar pode melhorar os resultados. O prompt deve especificar com clareza o que a LLM deve priorizar na resposta e o que deve ser ignorado. Isso orienta o modelo a focar no que Ã© mais relevante para sua aplicaÃ§Ã£o e seu objetivo.\n","\n","* Melhorar o prompt do usuÃ¡rio - lembrar o usuÃ¡rio (colocando um aviso na interface talvez) que quanto mais especÃ­fico for na pergunta maior a chance de aumentar a precisÃ£o das respostas geradas pela LLM. Quanto mais detalhado e claro o pedido, mais relevante serÃ¡ o retorno. Esta prÃ¡tica tambÃ©m ajuda a reduzir ambiguidades que podem prejudicar a interpretaÃ§Ã£o da consulta pelo modelo.\n","\n","* Ajustar o prompt de contextualizaÃ§Ã£o - lembrando que este prompt reformula a pergunta do usuÃ¡rio com base no histÃ³rico da conversa, algo Ãºtil quando a consulta precisa de contexto para ser corretamente interpretada. O prompt de contextualizaÃ§Ã£o (context_q_system_prompt) instrui o modelo a levar o histÃ³rico em consideraÃ§Ã£o; e embora o prompt atual esteja em inglÃªs devido Ã  maior chance de compatibilidade da LLM com este idioma (apesar de ser compatÃ­vel com o nosso), vocÃª pode testÃ¡-lo em portuguÃªs e assim fica fÃ¡cil modificar o texto para maximizar o desempenho no idioma desejado.\n","\n","* Testar outras LLMs - Explorar outros modelos de linguagem, especialmente aqueles que aceitam uma quantidade maior de tokens e tÃªm bom desempenho no idioma escolhido, pode melhorar a performance. Para casos mais exigentes, pode valer a pena considerar soluÃ§Ãµes proprietÃ¡rias como o ChatGPT ou serviÃ§os pagos (como o Groq, citado no Colab 1) que disponibilizam grandes modelos de cÃ³digo aberto. Modelos maiores podem lidar melhor com consultas complexas e fornecer respostas mais elaboradas.\n","\n","* Ajustar os parÃ¢metros de recuperaÃ§Ã£o (k e fetch_k) - Modificar os parÃ¢metros das etapas de recuperaÃ§Ã£o, como os valores de k e fetch_k, pode ter um impacto significativo no desempenho da sua aplicaÃ§Ã£o. Experimente comeÃ§ar com valores menores e aumentÃ¡-los conforme necessÃ¡rio, sempre monitorando o impacto na relevÃ¢ncia e qualidade das respostas. Para mais detalhes, consulte a seÃ§Ã£o da pipeline RAG e o retriever. Outra ideia seria testar outros algoritmos alÃ©m do MMR.\n","\n","* Deixar melhor preparado para aceitar qualquer documento - uma ideia Ã© fazer o preprocessamento de arquivos PDF (ou outros formatos) para adequaÃ§Ã£o ao vector store. Muitas vezes PDFs possuem tabelas ou outras estruturas que dificultam a interpretaÃ§Ã£o; ou ainda, documentos em formatos mais diferentes como HTML, CSV, ou PPTX nÃ£o estÃ£o estruturados para extraÃ§Ã£o ideal de informaÃ§Ãµes. A preparaÃ§Ã£o desses arquivos Ã© crucial para garantir que o conteÃºdo relevante seja corretamente capturado e disponibilizado para o sistema de recuperaÃ§Ã£o.\n"," * Existem soluÃ§Ãµes especializadas automatizam essa transformaÃ§Ã£o, organizando os dados e eliminando informaÃ§Ãµes desnecessÃ¡rias. Isso otimiza o fluxo de trabalho e melhora a precisÃ£o dos resultados. Um exemplo Ã© o serviÃ§o Unstructured (Acesse https://unstructured.io), que facilita a extraÃ§Ã£o de dados complexos de arquivos, tornando-os prontos para uso em bancos de dados vetoriais e frameworks de LLMs, o que aumenta a qualidade da recuperaÃ§Ã£o da informaÃ§Ã£o e o desempenho da aplicaÃ§Ã£o RAG.\n"," * Para usar isso no langchain Ã© simples, vocÃª pode usar o mÃ©todo de Document Loader. Na prÃ¡tica, basta carregar o documento usando o document loader Unstructured (ao invÃ©s do PyPDFLoader que usamos). Mais detalhes aqui: https://python.langchain.com/docs/integrations/document_loaders/unstructured_file/\n","\n","\n","Essas estratÃ©gias visam otimizar a eficiÃªncia e a qualidade das respostas do sistema RAG, adaptando-o ao seu caso de uso especÃ­fico.\n","\n","\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1ffUdoTetTjHOx3HIGi-0nvvg31e9cVYU","timestamp":1724761843583}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}