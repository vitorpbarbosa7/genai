{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOaSQlfiFLID"
   },
   "source": [
    "# Projeto 02 - Chatbot customizado com mem√≥ria e interface\n",
    "\n",
    "> Nesse projeto voc√™ aprender√° como adicionar melhorias ao sistema de chat que criamos, onde ser√° adicionado um sistema para que o bot seja capaz de lembrar  o hist√≥rico da conversa e com isso ter maior compreens√£o do contexto das mensagens, o que √© poss√≠vel atrav√©s das intera√ß√µes passadas (Chatbots com essa capacidade s√£o conhecidos como \"Context-Aware Chatbot\"). Al√©m disso, veremos como facilmente criar uma interface amig√°vel para sua aplica√ß√£o usando uma biblioteca vers√°til chamada Streamlit.\n",
    "\n",
    "Vamos aprender como fazer pelo Colab e tamb√©m como reaproveitar nosso c√≥digo para executar em seu ambiente local, o que pode ser mais interessante.\n",
    "\n",
    "Com a ideia de tornar nossa aplica√ß√£o mais flex√≠vel, deixaremos ela preparada para aceitar diferentes modelos e provedores de LLMs, tanto open source (com execu√ß√£o local ou via cloud) e propriet√°rias (via API).\n",
    "\n",
    "Desse modo, caso esteja usando pelo Colab, essa aplica√ß√£o vai funcionar inclusive se voc√™ selecionar CPU ao inv√©s de GPU.\n",
    "\n",
    " * Falaremos mais sobre a vantagem de cada m√©todo mais tarde, enquanto estivermos desenvolvendo a integra√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GauNtm2ws46r"
   },
   "source": [
    "## [ ! ] Como executar em ambiente local\n",
    "\n",
    "* Para executar o c√≥digo desse projeto em um ambiente local, siga as instru√ß√µes para instalar as depend√™ncias necess√°rias usando os comandos abaixo. Voc√™ pode usar os mesmos comandos de instala√ß√£o. Para mais detalhes, confira as aulas em v√≠deo referente √† configura√ß√£o local com o Streamlit.\n",
    "\n",
    "* Voc√™ pode executar localmente desde j√° conforme √© mostrado em aula - mas caso esteja com erros de configura√ß√£o em seu ambiente local, recomendamos fazer pelo Colab antes, para n√£o atrapalhar o fluxo de aprendizagem. Mas caso opte por fazer localmente j√° tamb√©m √© interessante, pois o Streamlit pede que trabalhemos com arquivo .py e aqui no Colab √© .ipynb por causa do Jupyter Notebook, portanto devemos juntar tudo num s√≥ arquivo .py  \n",
    "\n",
    "* Al√©m disso, ao executar em ambiente local voc√™ pode ir visualizando as altera√ß√µes mais rapidamente, pois ap√≥s executar o comando que inicializa o streamlit (exemplo: `!streamlit run projeto2.py`) basta que edite o script .py e salve, e recarregue a p√°gina do streamlit, assim ver√° a mudan√ßa. Ou seja, n√£o precisa reexecutar o comando `!streamlit...`)\n",
    "\n",
    "* Antes de rodar seu c√≥digo localmente, certifique-se de que todas as bibliotecas listadas no comando pip install estejam instaladas.  Caso ainda n√£o as tenha, voc√™ pode instal√°-las diretamente pelo terminal do VS Code (caso esteja usando essa IDE) ou o terminal/prompt normal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_CDI-qUEI0m"
   },
   "source": [
    "## Instala√ß√£o e Configura√ß√£o\n",
    "\n",
    "Precisamos instalar algumas bibliotecas que ser√£o necess√°rias em nossa aplica√ß√£o, como o LangChain e sstreamlit (para cria√ß√£o da interface), e mais alguns pacotes necess√°rios e que usamos anteriormente\n",
    "\n",
    "> Se estiver executando localmente: precisa instalar tamb√©m o pytorch, caso j√° n√£o tenha instalado (lembrando que no Colab j√° vem instalado por padr√£o, basta importar).\n",
    " * Para evitar problemas de compatibilidade, recomendamos esse comando: `pip install torch==2.3.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu121`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32914,
     "status": "ok",
     "timestamp": 1732997580522,
     "user": {
      "displayName": "Gabriel Alves",
      "userId": "07503873055563742982"
     },
     "user_tz": 180
    },
    "id": "CXD-F395R75e",
    "outputId": "95b525d9-028f-4a0a-bcfc-0f0f3724ddf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q streamlit langchain sentence-transformers\n",
    "!pip install -q langchain_community langchain-huggingface langchain_ollama langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0qSu4O8tXIh"
   },
   "source": [
    "> Instala√ß√£o do Localtunnel\n",
    "\n",
    "Caso esteja executando no Colab, voc√™ precisa tamb√©m instalar o Localtunnel para conseguirmos nos conectar √† aplica√ß√£o gerada com o streamlit.\n",
    "\n",
    "Isso ser√° explicado na etapa em que √© feita a inicializa√ß√£o da interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4563,
     "status": "ok",
     "timestamp": 1732997585078,
     "user": {
      "displayName": "Gabriel Alves",
      "userId": "07503873055563742982"
     },
     "user_tz": 180
    },
    "id": "ZdYajOzC-1Y6",
    "outputId": "ddd8d2dd-fe98-4b48-9281-46257b5820b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K\n",
      "added 22 packages in 4s\n",
      "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K\n",
      "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K3 packages are looking for funding\n",
      "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K  run `npm fund` for details\n",
      "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K"
     ]
    }
   ],
   "source": [
    "!npm install localtunnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQahRcTktkWu"
   },
   "source": [
    "### Carregando as vari√°veis de ambiente com o dotenv\n",
    "\n",
    "Utilizaremos a biblioteca dotenv, que simplifica a gest√£o de vari√°veis de ambiente ao armazen√°-las em um arquivo .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5324,
     "status": "ok",
     "timestamp": 1732997590394,
     "user": {
      "displayName": "Gabriel Alves",
      "userId": "07503873055563742982"
     },
     "user_tz": 180
    },
    "id": "7qWrf4IMAjrz",
    "outputId": "21baaf4e-2f91-48ed-dc9d-8cc9a3984ea0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82R6hEnbG98h"
   },
   "source": [
    "#### Cria√ß√£o do arquivo .env\n",
    "\n",
    "O comando `%%writefile` permite que a c√©lula do notebook seja salva como um arquivo externo, com o nome especificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1732997590395,
     "user": {
      "displayName": "Gabriel Alves",
      "userId": "07503873055563742982"
     },
     "user_tz": 180
    },
    "id": "UusxhnSpA4lL",
    "outputId": "b679dd00-4cce-469c-e71d-cc535b282e8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "HUGGINGFACE_API_KEY=\n",
    "HUGGINGFACEHUB_API_TOKEN=\n",
    "OPENAI_API_KEY=##########\n",
    "TAVILY_API_KEY=##########\n",
    "SERPAPI_API_KEY=##########\n",
    "LANGCHAIN_API_KEY=##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO2t7UV3HHiN"
   },
   "source": [
    "## Inicializa√ß√£o da interface\n",
    "\n",
    "Agora precisamos definir s√≥ algumas configura√ß√µes do Streamlit e ent√£o reunir todo o c√≥digo em um arquivo .py, desse modo conseguiremos rodar no Colab tamb√©m   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1732997590395,
     "user": {
      "displayName": "Gabriel Alves",
      "userId": "07503873055563742982"
     },
     "user_tz": 180
    },
    "id": "gC59Y_AF9Lu-",
    "outputId": "19552e0b-a0d4-4956-be66-1308d6d2368e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing projeto2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile projeto2.py\n",
    "\n",
    "import streamlit as st\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import torch\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configura√ß√µes do Streamlit\n",
    "st.set_page_config(page_title=\"Seu assistente virtual ü§ñ\", page_icon=\"ü§ñ\")\n",
    "st.title(\"Seu assistente virtual ü§ñ\")\n",
    "\n",
    "model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n",
    "\n",
    "def model_hf_hub(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", temperature=0.1):\n",
    "  llm = HuggingFaceHub(\n",
    "      repo_id=model,\n",
    "      model_kwargs={\n",
    "          \"temperature\": temperature,\n",
    "          \"return_full_text\": False,\n",
    "          \"max_new_tokens\": 512,\n",
    "          #\"stop\": [\"<|eot_id|>\"],\n",
    "          # demais par√¢metros que desejar\n",
    "      }\n",
    "  )\n",
    "  return llm\n",
    "\n",
    "def model_openai(model=\"gpt-4o-mini\", temperature=0.1):\n",
    "    llm = ChatOpenAI(\n",
    "        model=model,\n",
    "        temperature=temperature\n",
    "        # demais par√¢metros que desejar\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def model_ollama(model=\"phi3\", temperature=0.1):\n",
    "    llm = ChatOllama(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "def model_response(user_query, chat_history, model_class):\n",
    "\n",
    "    ## Carregamento da LLM\n",
    "    if model_class == \"hf_hub\":\n",
    "        llm = model_hf_hub()\n",
    "    elif model_class == \"openai\":\n",
    "        llm = model_openai()\n",
    "    elif model_class == \"ollama\":\n",
    "        llm = model_ollama()\n",
    "\n",
    "    ## Defini√ß√£o dos prompts\n",
    "    system_prompt = \"\"\"\n",
    "    Voc√™ √© um assistente prestativo e est√° respondendo perguntas gerais. Responda em {language}.\n",
    "    \"\"\"\n",
    "    # corresponde √† vari√°vel do idioma em nosso template\n",
    "    language = \"portugu√™s\"\n",
    "\n",
    "    # Adequando √† pipeline\n",
    "    if model_class.startswith(\"hf\"):\n",
    "        user_prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    else:\n",
    "        user_prompt = \"{input}\"\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", user_prompt)\n",
    "    ])\n",
    "\n",
    "    ## Cria√ß√£o da Chain\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    ## Retorno da resposta / Stream\n",
    "    return chain.stream({\n",
    "        \"chat_history\": chat_history,\n",
    "        \"input\": user_query,\n",
    "        \"language\": language\n",
    "    })\n",
    "\n",
    "\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = [\n",
    "        AIMessage(content=\"Ol√°, sou o seu assistente virtual! Como posso ajudar voc√™?\"),\n",
    "    ]\n",
    "\n",
    "for message in st.session_state.chat_history:\n",
    "    if isinstance(message, AIMessage):\n",
    "        with st.chat_message(\"AI\"):\n",
    "            st.write(message.content)\n",
    "    elif isinstance(message, HumanMessage):\n",
    "        with st.chat_message(\"Human\"):\n",
    "            st.write(message.content)\n",
    "\n",
    "user_query = st.chat_input(\"Digite sua mensagem aqui...\")\n",
    "if user_query is not None and user_query != \"\":\n",
    "    st.session_state.chat_history.append(HumanMessage(content=user_query))\n",
    "\n",
    "    with st.chat_message(\"Human\"):\n",
    "        st.markdown(user_query)\n",
    "\n",
    "    with st.chat_message(\"AI\"):\n",
    "        resp = st.write_stream(model_response(user_query, st.session_state.chat_history, model_class))\n",
    "        print(st.session_state.chat_history)\n",
    "\n",
    "    st.session_state.chat_history.append(AIMessage(content=resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jrLVCt24tDC"
   },
   "source": [
    "### Execu√ß√£o do Streamlit\n",
    "\n",
    "Tendo nosso script pronto, basta executar o comando abaixo para rodar a nossa aplica√ß√£o pelo streamlit.\n",
    "Isso far√° com que a aplica√ß√£o do Streamlit seja executada em segundo plano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1732997590395,
     "user": {
      "displayName": "Gabriel Alves",
      "userId": "07503873055563742982"
     },
     "user_tz": 180
    },
    "id": "xDzu4rHv9T3f"
   },
   "outputs": [],
   "source": [
    "!streamlit run projeto2.py &>/content/logs.txt &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COvIzXDGvpjI"
   },
   "source": [
    "\n",
    "\n",
    "Observa√ß√£o:\n",
    "* O `&` no final permite que o Colab continue executando outras c√©lulas sem esperar que o aplicativo Streamlit termine.\n",
    "\n",
    "* ao rodar localmente n√£o √© necess√°rio o `&>/content/logs.txt &`\n",
    "\n",
    "* aqui usamos pois o Colab n√£o exibe no terminal a informa√ß√£o que precisamos, pois n√£o podemos visualiz√°-lo pelo Colab (j√° que ele funciona de outro modo e n√£o temos acesso ao terminal que √© atualizado em tempo real - pelo menos na vers√£o gratuita).\n",
    "\n",
    "\n",
    "* O que esse trecho faz portanto √© adicionar os logs do comando a um arquivo chamado `logs.txt`\n",
    "\n",
    "\n",
    ">  Caso esteja acessando localmente agora basta acessar o link que ir√° aparecer no terminal (local URL ou Network URL, caso esteja em outro dispositivo na mesma rede).\n",
    " *  Para o Colab, precisa de mais um comando para abrir nossa aplica√ß√£o (veja abaixo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ggfAm5wCDBs"
   },
   "source": [
    "### Acesso com LocalTunnel\n",
    "\n",
    "Antes de conectar com o localtunnel, voc√™ precisa obter o IP externo, que ser√° usado como a senha ao fazer o launch da aplica√ß√£o nessa pr√≥xima etapa.\n",
    "\n",
    "Tem duas maneiras de fazer isso:\n",
    "\n",
    "1) com o comando abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1732997590721,
     "user": {
      "displayName": "Gabriel Alves",
      "userId": "07503873055563742982"
     },
     "user_tz": 180
    },
    "id": "h2HtjvqKGMMr",
    "outputId": "a9d5657e-ec36-41f3-d272-5e5ea03744c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.83.187.104\n"
     ]
    }
   ],
   "source": [
    "!wget -q -O - ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaA_NWZ4vxkE"
   },
   "source": [
    "2) Ou, como alternativa, fa√ßa desse modo:\n",
    "\n",
    " * Abra o painel lateral do Colab\n",
    " * Clique sobre o arquivo logs.txt. Aqui mostra o que seria exibido no terminal\n",
    " * Selecione o n√∫mero IP correspondente ao External URL. Somente o n√∫mero IP com os pontos, sem o http:// ou a porta\n",
    "  * Por exemplo: `35.184.1.10`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSkMYtN4zLnp"
   },
   "source": [
    "Pronto, agora basta executar o comando abaixo.\n",
    "\n",
    "Esse comando usa npx localtunnel para \"expor\" o aplicativo Streamlit em execu√ß√£o local para a internet. O aplicativo √© hospedado na porta 8501, e o localtunnel fornece uma URL p√∫blica por meio da qual o aplicativo pode ser acessado.\n",
    "\n",
    "Ent√£o, entre no link que aparece na sa√≠da e informar o IP no campo Tunnel Password. Logo em seguida, clique no bot√£o e aguarde o interface ser inicializada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110345,
     "status": "ok",
     "timestamp": 1732997723701,
     "user": {
      "displayName": "Gabriel Alves",
      "userId": "07503873055563742982"
     },
     "user_tz": 180
    },
    "id": "5EqkzAQE-5rp",
    "outputId": "1b4c83b7-6915-4ed5-acfe-160c9d2af3df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0Kyour url is: https://yellow-apples-divide.loca.lt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaj0JGZ4w0I-"
   },
   "source": [
    "Observa√ß√£o:\n",
    " * Se der algum erro, recarregue a p√°gina e aguarde mais alguns instantes.\n",
    " * Caso esteja usando um m√©todo que n√£o seja por API ent√£o √© normal que na primeira execu√ß√£o leve um pouco mais de tempo.\n",
    "  * Se velocidade for um fator muito determinante, recomendamos usar solu√ß√µes onde o processamento √© feito em um servidor externo e conecta-se via API como HF, Open AI ou Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojxqLY40kZ_Q"
   },
   "source": [
    "---\n",
    "\n",
    "## Criando seu pr√≥prio prompt\n",
    "> **Dica de estrutura para criar seu pr√≥prio Prompt**\n",
    "\n",
    "Voc√™ pode modificar √† vontade o prompt para que atenda ao seu objetivo. Voc√™ pode usar esse formato:\n",
    "\n",
    "* Introdu√ß√£o: Comece com uma breve introdu√ß√£o ao tema, definindo o conceito b√°sico.\n",
    "* Explica√ß√£o: Forne√ßa uma explica√ß√£o detalhada, mas simples, sobre o conceito. Utilize exemplos pr√°ticos ou analogias quando necess√°rio para facilitar a compreens√£o.\n",
    "* Passos ou Componentes: Se o conceito tiver v√°rios componentes ou etapas, liste e explique cada um de forma concisa.\n",
    "* Aplica√ß√µes: D√™ exemplos de como esse conceito √© aplicado na pr√°tica ou em contextos reais.\n",
    "* Resumo: Conclua com um resumo das principais ideias apresentadas.\n",
    "* Orienta√ß√µes Adicionais: Caso seja relevante, ofere√ßa dicas ou orienta√ß√µes adicionais para aprofundamento no tema.\n",
    "\n",
    "Palavras-chave relevantes para adicionar ao seu prompt e informar como deseja que seja sua resposta:\n",
    "* Claro, Objetivo, Simples, Exemplo pr√°tico, Analogia, Explica√ß√£o detalhada, Resumo\n",
    "\n",
    "Outras ideias:\n",
    "* explique [x] para algu√©m leigo; explique de modo f√°cil como se tivesse explicando para uma crian√ßa.\n",
    "\n",
    "Indo al√©m:\n",
    "* voc√™ pode tamb√©m procurar frameworks de prompt para fazer com que LLM desempenhe da melhor forma o papel desejado. Por exemplo, o framwork [COSTAR](https://medium.com/@frugalzentennial/unlocking-the-power-of-costar-prompt-engineering-a-guide-and-example-on-converting-goals-into-dc5751ce9875), m√©todo que garante que todos os aspectos-chave que influenciam a resposta de um LLM sejam considerados, resultando em respostas de sa√≠da mais personalizadas.\n",
    "* Quando o objetivo √© fazer com que o modelo desempenhe um papel espec√≠fico ou atue de um determinado modo, √© chamado de role-playing, e tem crescido muito as pesquisas em cima disso (como por exemplo [esse paper](https://arxiv.org/abs/2406.00627)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DL3ie4MprOha"
   },
   "source": [
    "## Alternativa ao Streamlit\n",
    "\n",
    "Criando nossa pr√≥pria aplica√ß√£o com o streamlit nos garante uma certa liberdade, principalmente porque ao criar \"do zero\" podemos deixar do jeito que queremos. Mas existem outras formas mais prontas e com a interface j√° criada e dispon√≠vel para uso, n√£o exigindo lidar com c√≥digo. Como nossa inten√ß√£o aqui √© tamb√©m trabalhar com o c√≥digo fonte e n√£o depender unicamente de um programa/interface ent√£o n√£o acabamos abordando, mas caso tenha interesse de usar uma alternativa assim ent√£o temos algumas recomenda√ß√µes:\n",
    "\n",
    "* Open WebUI - https://github.com/open-webui/open-webui\n",
    "* GPT4All - https://gpt4all.io/index.html\n",
    "* AnythingLLM - https://anythingllm.com\n",
    "\n",
    "Essas solu√ß√µes possuem v√°rias outras funcionalidades interessantes e integra√ß√µes, portanto pode ser uma boa ideia checar caso tenha interesse em explorar mais LLMs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Y-vZbs_sEa2Y5wq0jbqSoQ60w5OJEAv2",
     "timestamp": 1725397980979
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
