{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MFa7QY1mkzBx"
   },
   "source": [
    "# Projeto 01 - Transcri√ß√£o e compreens√£o de v√≠deos\n",
    "\n",
    "Neste projeto, vamos aprender a realizar transcri√ß√£o e compreens√£o de v√≠deos. Ao final, voc√™ ser√° capaz de criar sua pr√≥pria aplica√ß√£o que faz a sumariza√ß√£o autom√°tica de v√≠deos, permitindo que voc√™ entenda do que se trata e o que foi falado nele sem precisar assist√≠-lo.\n",
    "\n",
    "Objetivos deste projeto:\n",
    "\n",
    "* Compreender o conte√∫do de um v√≠deo do Youtube sem precisar assisti-lo.\n",
    "* Pesquisar informa√ß√µes √∫teis no v√≠deo sem perder nenhum detalhe importante.\n",
    "* Interagir com o conte√∫do do v√≠deo por meio de uma interface de chat (ou seja, como \"conversar com o v√≠deo\").\n",
    "\n",
    "## Instala√ß√£o e Configura√ß√£o\n",
    "\n",
    "#!pip install -q langchain_core langchain_community langchain-huggingface langchain_ollama langchain_openai\n",
    "!pip install -q langchain_core==0.3.32 langchain_community==0.3.15 langchain-huggingface langchain_ollama langchain_openai\n",
    "\n",
    "### Instala√ß√£o de bibliotecas para baixar transcri√ß√£o\n",
    "\n",
    "> **YouTube Transcript API**\n",
    "\n",
    "Esta √© uma API python que permite que voc√™ obtenha a transcri√ß√£o/legendas para um determinado v√≠deo do YouTube. Ela tamb√©m funciona para legendas geradas automaticamente e possui suporta a uma fun√ß√£o que faz automaticamente a tradu√ß√£o de legendas\n",
    "\n",
    "!pip install youtube-transcript-api==0.6.3\n",
    "\n",
    "> **pytube**\n",
    "\n",
    "Tamb√©m √© uma biblioteca que auxilia com o download de v√≠deos no youtube. Aqui ela n√£o √© necess√°ria para baixar as transcri√ß√µes dos v√≠deos, conseguimos ter acesso sem ela, mas iremos instalar tamb√©m pois com ela podemos recuperar tamb√©m demais informa√ß√µes do v√≠deo, como t√≠tulo, data de publica√ß√£o, descri√ß√£o, etc.\n",
    "\n",
    "!pip install pytube\n",
    "\n",
    "## Importa√ß√µes\n",
    "\n",
    "import os\n",
    "import io\n",
    "import getpass\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "## Carregando a transcri√ß√£o\n",
    "\n",
    "Para fazer o carregamento das transcri√ß√µes usaremos o m√©todo YoutubeLoader(), que faz parte do dos document_loaders do LangChain. Veremos logo em seguida tamb√©m como extrair os metadados do v√≠deo usando essa fun√ß√£o\n",
    "\n",
    "Atrav√©s desses m√©todo conseguimos puxar as transcri√ß√µes que j√° est√£o associadas ao v√≠deo e armazenadas no banco de dados do Youtube, o que ir√° nos economizar bastante processamento.\n",
    "\n",
    "\n",
    "O primeiro par√¢metro √© a URL do v√≠deo que queremos realizar a transcri√ß√£o\n",
    "\n",
    "Vamos pegar esse v√≠deo como primeiro exemplo https://www.youtube.com/watch?v=II28i__Tf3M\n",
    "\n",
    "### Defininido idiomas\n",
    "\n",
    "O segundo par√¢metro √© o language. A fun√ß√£o espera uma lista, nesse caso, uma lista de c√≥digos de idioma em prioridade decrescente (por padr√£o).\n",
    "\n",
    "Al√©m de ingl√™s (\"en\"), recomendamos deixar antes \"pt\" e \"pt-BR\" (ou \"pt-PT\") pois em alguns v√≠deos n√£o possui \"pt\". Embora a grande maioria dos v√≠deos que testamos possua legenda com c√≥digo \"pt\", mesmo para v√≠deos com a legenda em portugu√™s brasileiro. Ou seja, deixamos assim pois em alguns v√≠deos do portugu√™s brasileiro por exemplo o c√≥digo √© \"pt\", j√° para outros est√° como \"pt-BR\".\n",
    "\n",
    "\n",
    "video_loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=II28i__Tf3M\",\n",
    "                                              language = [\"pt\", \"pt-BR\", \"en\"],)\n",
    "\n",
    "Usaremos o .load() para fazer a leitura e ao mesmo tempo podemos passar as informa√ß√µes do v√≠deo para uma vari√°vel\n",
    "\n",
    "infos = video_loader.load()\n",
    "infos\n",
    "\n",
    "O valor de \"page_content\" corresponde √† transcri√ß√£o em si\n",
    "\n",
    "para acess√°-la devemos colocar `[0]` pois `infos` √© uma lista, que nesse caso s√≥ tem o primeiro valor. O c√≥digo ent√£o fica assim\n",
    "\n",
    "transcricao = infos[0].page_content\n",
    "transcricao\n",
    "\n",
    "Esse primeiro exemplo √© de uma legenda que foi gerada automaticamente pelo sistema de reconhecimento de fala do youtube, que no geral tende a ser bom mas pode gerar erros, ent√£o n√£o √© perfeito. Mas ainda assim, dependendo da LLM ela vai entender que se trata de um erro com base no contexto\n",
    "\n",
    "Para legendas autom√°ticas verificamos que n√£o houve perda consider√°vel na compreens√£o, mas obviamento √© esperado que uma legenda feita manualmente possua maiores chances de resultados melhores\n",
    "\n",
    "### Obter informa√ß√µes do v√≠deo\n",
    "\n",
    "Note que carregamos a legenda/transcri√ß√£o mas nenhuma outra informa√ß√£o sobre o v√≠deo, o que pode ser √∫til depedendo do nosso objetivo.\n",
    "\n",
    "> H√° duas maneiras de obter informa√ß√µes do v√≠deo:\n",
    "\n",
    "1. Usar o par√¢metro `add_video_info` na fun√ß√£o YoutubeLoader.from_youtube_url(). Por tr√°s dos panos, a fun√ß√£o se comunica com a biblioteca pytube. No entanto, h√° um bug conhecido com o pytube que pode ocorrer ocasionalmente e que at√© o momento n√£o foi definitivamente resolvido. Obs: Esse bug j√° existe h√° algum tempo, sem um prazo claro para uma corre√ß√£o completa pelos autores. √â por isso que mostraremos um segundo m√©todo.\n",
    "\n",
    "2. Usar a biblioteca `BeautifulSoup` (ou similar). Embora essa abordagem seja um pouco mais manual, mostramos como ela ainda pode ser feita facilmente com apenas algumas linhas de c√≥digo.\n",
    "\n",
    "\n",
    "\n",
    "#### m√©todo 1: Com par√¢metro add_video_info\n",
    "\n",
    "Podemos passar como par√¢metro add_video_info=True (que por padr√£o √© =False) e isso far√° com que sejam retornados os metadados do v√≠deo, como: t√≠tulo, descri√ß√£o, autor, visualiza√ß√µes, e capa)\n",
    "\n",
    "Para usar esse par√¢metro voc√™ precisa ter instalado antes a biblioteca pytube (lembre-se que instalamos anteriormente neste Colab, ent√£o se estiver executando em seu computador certifique-se que instalou essa biblioteca)\n",
    "\n",
    "video_loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=II28i__Tf3M\",\n",
    "                                               #add_video_info = True,\n",
    "                                               language = [\"pt\", \"pt-BR\", \"en\"],)\n",
    "\n",
    "infos = video_loader.load()\n",
    "\n",
    "infos\n",
    "\n",
    "Podemos organizar desse modo\n",
    "\n",
    "infos_video = f\"\"\"Informa√ß√µes do v√≠deo:\n",
    "\n",
    "T√≠tulo: {infos[0].metadata['title']}\n",
    "Autor: {infos[0].metadata['author']}\n",
    "Data: {infos[0].metadata['publish_date'][:10]}\n",
    "URL: https://www.youtube.com/watch?v={infos[0].metadata['source']}\n",
    "\n",
    "Transcri√ß√£o: {transcricao}\n",
    "\"\"\"\n",
    "print(infos_video)\n",
    "\n",
    "> **Observa√ß√£o importante:** Se aparecer esse erro para voc√™ \"PytubeError: Exception while accessing title of...\" tente reiniciar a sess√£o do Colab e executar novamente o mesmo c√≥digo na ordem. Essa mensagem √© referente a um bug da biblioteca usada para ler os dados do Youtube e que at√© o momento n√£o h√° um padr√£o conhecido ou uma explica√ß√£o dos autores (se esse bug for definitivamente resolvido iremos remover esse aviso). Se esse erro continuar a aparecer para voc√™, use o m√©todo 2 abaixo\n",
    "\n",
    "#### m√©todo 2: Com BeautifulSoup\n",
    "\n",
    "Para fazer desse modo, precisaremos importar a biblioteca `requests` e o m√©todo `bs4` do `beautifulsoup`, que j√° est√° instalado por padr√£o no Colab. Ent√£o, ao inv√©s do `infos = video_loader.load()` (conforme explicado no m√©todo 1 acima) voc√™ usar√° esse c√≥digo abaixo\n",
    "* explica√ß√£o r√°pida sobre o c√≥digo: √© feita a busca no conte√∫do HTML da p√°gina do v√≠deo no YouTube usando a fun√ß√£o `requests.get()` e, em seguida, analisa-o com o BeautifulSoup. Ele procura a tag usando `soup.find_all()`, que retorna uma lista de tags correspondentes. O t√≠tulo do v√≠deo √© extra√≠do pegando o primeiro elemento, convertendo-o em uma string e, em seguida, removendo o `<title>` e as tags com `replace()`. Ao final, o c√≥digo imprime o t√≠tulo \"limpo\".\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_video_title(url):\n",
    "  r = requests.get(url)\n",
    "  soup = BeautifulSoup(r.text)\n",
    "\n",
    "  link = soup.find_all(name=\"title\")[0]\n",
    "  title = str(link)\n",
    "  title = title.replace(\"<title>\",\"\")\n",
    "  title = title.replace(\"</title>\",\"\")\n",
    "\n",
    "  return title\n",
    "\n",
    "video_url = \"https://www.youtube.com/watch?v=II28i__Tf3M\"\n",
    "\n",
    "video_title = get_video_title(video_url)\n",
    "video_title\n",
    "\n",
    "### Reunindo as informa√ß√µes\n",
    "\n",
    "Agora, vamos apenas combinar as informa√ß√µes do v√≠deo com a transcri√ß√£o que obtivemos anteriormente. Salvaremos tudo em uma mesma vari√°vel chamada `infos_video`\n",
    "\n",
    "infos_video = f\"\"\"Informa√ß√µes do V√≠deo:\n",
    "\n",
    "T√≠tulo: {video_title}\n",
    "URL: {video_url}\n",
    "\n",
    "Transcri√ß√£o: {transcricao}\n",
    "\"\"\"\n",
    "print(infos_video)\n",
    "\n",
    "## Salvando transcri√ß√£o em um arquivo\n",
    "\n",
    "Esse c√≥digo abre um arquivo chamado \"transcricao.txt\" em modo de escrita (\"w\") com codifica√ß√£o UTF-8;  dentro do bloco `with` ele grava dados no arquivo.\n",
    "\n",
    "Para cada item na vari√°vel `infos`, ele escreve o conte√∫do da vari√°vel `infos_video` no arquivo. O uso do bloco with garante que o arquivo seja fechado corretamente ap√≥s a grava√ß√£o, mesmo que ocorra algum erro durante a execu√ß√£o. E aqui n√£o precisa do `f.close()` pois o bloco with fecha o arquivo automaticamente ao finalizar\n",
    "\n",
    "with io.open(\"transcricao.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "  for doc in infos:\n",
    "    f.write(infos_video)\n",
    "\n",
    "## Carregamento do modelo\n",
    "\n",
    "Vamos reaproveitar as fun√ß√µes de carregamento que usamos nos projeos anteriores, basta copiar e colar\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "def model_hf_hub(model = \"meta-llama/Meta-Llama-3-8B-Instruct\", temperature = 0.1):\n",
    "  llm = HuggingFaceEndpoint(repo_id = model,\n",
    "                       temperature = temperature,\n",
    "                       return_full_text = False,\n",
    "                       max_new_tokens = 1024,\n",
    "                       task=\"text-generation\"\n",
    "                       )\n",
    "  return llm\n",
    "\n",
    "def model_openai(model = \"gpt-4o-mini\", temperature = 0.1):\n",
    "  llm = ChatOpenAI(model = model, temperature = temperature)\n",
    "  return llm\n",
    "\n",
    "def model_ollama(model = \"phi3\", temperature = 0.1):\n",
    "  llm = ChatOllama(model = model, temperature = temperature)\n",
    "  return llm\n",
    "\n",
    "E aqui no Colab precisar setar as vari√°veis de ambiente. Pode usar o .env tamb√©m, especialmente se estiver executando localmente\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass()\n",
    "os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n",
    "\n",
    "if model_class == \"hf_hub\":\n",
    "  llm = model_hf_hub()\n",
    "elif model_class == \"openai\":\n",
    "  llm = model_openai\n",
    "elif model_class == \"ollama\":\n",
    "  llm = model_ollama\n",
    "\n",
    "\n",
    "model_class, llm\n",
    "\n",
    "## Cria√ß√£o do prompt template\n",
    "\n",
    "Vamos manter a base do prompt simples, basicamente instruindo que deve responder com base na transcri√ß√£o fornecida. Voc√™ pode modific√°-lo √† vontade depois, para deixar mais adequado ao seu objetivo ou simplesmente para tentar alcan√ßar melhores resultados\n",
    "\n",
    "* Aqui vamos passar o transcri√ß√£o completa. Estaremos lidando com modelos que possuem uma janela grande de contexto - por exemplo o llama 3 possui algo em torno de 8k, j√° o chatGPT 4o por exemplo possui ainda mais. Deve ser uma capacidade suficiente de leitura de tokens de entrada para lidar com a maioria das transcri√ß√µes dos v√≠deos, e ser√° para todos os testados aqui.\n",
    "* Como a ideia desse projeto √© criar uma ferramenta que faz o resumo / sumariza√ß√£o ent√£o adicionar a transcri√ß√£o inteira como contexto √© at√© uma op√ß√£o mais interessante, j√° que para RAG √© recuperado geralmente uma quantidade limite de peda√ßos de documento. Portanto, se fosse usado RAG teria que configurar bem os par√¢metros, provavelmente escolher um valor maior de k por exemplo para recuperar mais documentos (no entanto, lembre-se que elevar muito esse valor aumenta o custo computacional da aplica√ß√£o)\n",
    "* Mas caso o v√≠deo seja realmente grande ent√£o pode ser interessante dividir em partes. Para isso sugerimos usar o c√≥digo do projeto 3, pode copiar as fun√ß√µes prontas que fazem as etapas de indexa√ß√£o e recupera√ß√£o (indexing & retrieval)\n",
    "\n",
    "Al√©m da transcri√ß√£o, o prompt template ir√° aceitar a vari√°vel consulta, que nada mais √© do que a entrada para a LLM, que pode ser uma pergunta ou instru√ß√£o\n",
    "\n",
    "E o `if model_class.startswith(\"hf\"):` apenas copiamos do projeto anterior, lembrando que isso √© para melhorar os resultados com a implementa√ß√£o via Hugging Face Hub, que at√© o momento funciona melhor se especificarmos manualmente os tokens de in√≠cio e fim. Aqui o template √© do llama 3.x, mas for usar outro modelo open source que exija template diferente ent√£o lembre de mudar.\n",
    "\n",
    "\n",
    "system_prompt = \"Voc√™ √© um assistente virtual prestativo e deve responder a uma consulta com base na transcri√ß√£o de um v√≠deo, que ser√° fornecida abaixo.\"\n",
    "\n",
    "inputs = \"Consulta: {consulta} \\n Transcri√ß√£o: {transcricao}\"\n",
    "\n",
    "if model_class.startswith(\"hf\"):\n",
    "  user_prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\".format(inputs)\n",
    "else:\n",
    "  user_prompt = \"{}\".format(inputs)\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_prompt), (\"user\", user_prompt)])\n",
    "\n",
    "prompt_template\n",
    "\n",
    "## Cria√ß√£o da chain\n",
    "\n",
    "Nossa chain ficar√° assim\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "## Gera√ß√£o da resposta\n",
    "\n",
    "Por fim, vamos gerar o resultado, fornecendo como par√¢metro a transcri√ß√£o e a consulta que queremos (podendo ser pergunta, instru√ß√£o, etc.)\n",
    "\n",
    "res = chain.invoke({\"transcricao\": transcricao, \"consulta\": \"resuma\"})\n",
    "print(res)\n",
    "\n",
    "Podemos melhorar esse prompt (consulta), deixando algo como `\"sumarize de forma clara de entender`\n",
    "\n",
    "res = chain.invoke({\"transcricao\": transcricao, \"consulta\": \"sumarize de forma clara de entender\"})\n",
    "print(res)\n",
    "\n",
    "res = chain.invoke({\"transcricao\": transcricao, \"consulta\": \"explique em 1 frase sobre o que fala esse v√≠deo\"})\n",
    "print(res)\n",
    "\n",
    "res = chain.invoke({\"transcricao\": transcricao, \"consulta\": \"liste os temas desse video\"})\n",
    "print(res)\n",
    "\n",
    "## Tradu√ß√£o da transcri√ß√£o\n",
    "\n",
    "Para os modelos mais modernos n√£o √© necess√°rio traduzir antes, pode carregar a transcri√ß√£o no idioma desejado e passar para a LLM mesmo que no idioma diferente daquele que voc√™ escreveu as instru√ß√µes no prompt template, isso porque o modelo deve ser capaz de entender.\n",
    "\n",
    "Mas tamb√©m √© poss√≠vel traduzir a transcri√ß√£o usando essa mesma ferramenta.\n",
    "Isso pode ser muito √∫til caso o modelo que esteja trabalhando n√£o funcione bem para m√∫ltiplos idiomas.\n",
    "\n",
    "Para implementar isso, basta definirmos para o par√¢metro translation o c√≥digo do idioma para o qual desejamos traduzir.\n",
    "Por exemplo para o franc√™s ficaria assim\n",
    "\n",
    "url_video = \"https://www.youtube.com/watch?v=II28i__Tf3M\"\n",
    "\n",
    "video_loader = YoutubeLoader.from_youtube_url(\n",
    "    url_video,\n",
    "    add_video_info=True,\n",
    "    language=[\"pt\", \"en\"],\n",
    "    translation=\"fr\",\n",
    ")\n",
    "\n",
    "infos = video_loader.load()\n",
    "transcricao = infos[0].page_content\n",
    "transcricao\n",
    "\n",
    "## Jun√ß√£o da pipeline em fun√ß√µes\n",
    "\n",
    "Para deixar mais pr√°tico e evitar repeti√ß√µes do c√≥digo vamos reunir toda a nossa l√≥gica em fun√ß√µes, assim n√£o vai ser mais necess√°rio ficar copiando e colando o c√≥digo toda vez que for testar em outro v√≠deo\n",
    "\n",
    "def llm_chain(model_class):\n",
    "  system_prompt = \"Voc√™ √© um assistente virtual prestativo e deve responder a uma consulta com base na transcri√ß√£o de um v√≠deo, que ser√° fornecida abaixo.\"\n",
    "\n",
    "  inputs = \"Consulta: {consulta} \\n Transcri√ß√£o: {transcricao}\"\n",
    "\n",
    "  if model_class.startswith(\"hf\"):\n",
    "      user_prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\".format(inputs)\n",
    "  else:\n",
    "      user_prompt = \"{}\".format(inputs)\n",
    "\n",
    "  prompt_template = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_prompt),\n",
    "      (\"user\", user_prompt)\n",
    "  ])\n",
    "\n",
    "  ### Carregamento da LLM\n",
    "  if model_class == \"hf_hub\":\n",
    "      llm = model_hf_hub()\n",
    "  elif model_class == \"openai\":\n",
    "      llm = model_openai()\n",
    "  elif model_class == \"ollama\":\n",
    "      llm = model_ollama()\n",
    "\n",
    "  chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "  return chain\n",
    "\n",
    "def get_video_info(url_video, language=\"pt\", translation=None):\n",
    "\n",
    "  video_loader = YoutubeLoader.from_youtube_url(\n",
    "      url_video,\n",
    "      #add_video_info=False,\n",
    "      language=language,\n",
    "      translation=translation,\n",
    "  )\n",
    "\n",
    "  infos = video_loader.load()[0]\n",
    "  transcript = infos.page_content\n",
    "  video_title = get_video_title(url_video)\n",
    "\n",
    "  return transcript, metadata\n",
    "\n",
    "Vamos testar abaixo\n",
    "\n",
    "transcript, metadata = get_video_info(\"https://www.youtube.com/watch?v=II28i__Tf3M\")\n",
    "\n",
    "metadata, transcript\n",
    "\n",
    "Aqui aproveitamos para adicionar um Tratamento de erro com Try Catch, pois caso n√£o haja uma transcri√ß√£o para esse v√≠deo ser√° retornado um erro (possivelmente ser√° esse: `IndexError: list index out of range`), com isso n√£o ser√° poss√≠vel fazer os processamentos seguintes. Por isso programos aqui para o programa parar interromper a execu√£o se esse for caso\n",
    "\n",
    "def interpret_video(url, query=\"resuma\", model_class=\"hf_hub\", language=\"pt\", translation=None):\n",
    "\n",
    "  try:\n",
    "    transcript, metadata = get_video_info(url, language, translation)\n",
    "\n",
    "    chain = llm_chain(model_class)\n",
    "\n",
    "    res = chain.invoke({\"transcricao\": transcript, \"consulta\": query})\n",
    "    print(res)\n",
    "\n",
    "  except Exception as e:\n",
    "    print(\"Erro ao carregar transcri√ß√£o\")\n",
    "    print(e)\n",
    "\n",
    "## Gera√ß√£o final\n",
    "\n",
    "Podemos definir uma interface mais apresent√°vel no Colab atrav√©s dos comandos para deixar as vari√°veis em formato de valores de formul√°rio.\n",
    "\n",
    "Ideias do que adicionar √† query:\n",
    "\n",
    "* `sumarize de forma clara de entender`\n",
    "* `liste os temas desse v√≠deo`\n",
    "* `explique em 1 frase sobre o que fala esse v√≠deo`\n",
    "\n",
    "url_video = \"https://www.youtube.com/watch?v=II28i__Tf3M\" # @param {type:\"string\"}\n",
    "query_user = \"sumarize de forma clara de entender\" # @param {type:\"string\"}\n",
    "model_class = \"openai\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n",
    "language = [\"pt\", \"pt-BR\", \"en\"] # @param {type:\"string\"}\n",
    "\n",
    "interpret_video(url_video, query_user, model_class, language)\n",
    "\n",
    "url_video = \"https://www.youtube.com/watch?v=rEE8ERGKsqo\" # @param {type:\"string\"}\n",
    "query_user = \"sumarize de forma clara de entender\" # @param {type:\"string\"}\n",
    "model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n",
    "language = [\"pt\", \"pt-BR\", \"en\"] # @param {type:\"string\"}\n",
    "\n",
    "interpret_video(url_video, query_user, model_class, language)\n",
    "\n",
    "## Explorando mais\n",
    "\n",
    "Vamos deixar nossa aplica√ß√£o mais interessante. Podemos fazer mais de uma consulta/query de uma vez por v√≠deo.\n",
    "\n",
    "* Para cada v√≠deo informado, podemos definir para exibir informa√ß√µes como o titulo dele;\n",
    "* e logo abaixo um resumo em um paragrafo;\n",
    "* depois uma lista de temas abordados, etc.\n",
    "\n",
    "No Colab temos um modo interessante de fazer isso, que √© atrav√©s do Markdown.\n",
    "\n",
    "\n",
    "### Exemplo com Markdown\n",
    "\n",
    "Markdown √© um formato de linguagem de marca√ß√£o muito adotada pela comunidade por ser simples e leve, permitindo criar texto formatado com uso de s√≠mbolos especiais, como asteriscos e colchetes, em vez de tags de HTML. No Google Colab, o uso de markdown pode tornar a visualiza√ß√£o do texto mais interessante e f√°cil de ler, facilitando a compreens√£o e a apresenta√ß√£o de informa√ß√µes. Por exemplo, usando markdown, voc√™ pode deixar um texto em *it√°lico* ao deixar dentro de asteriscos, ou **negrito** se deixar ele dentro de asteriscos duplos. Tamb√©m podemos adicionar t√≠tulos com diferentes n√≠veis, por exemplo para n√≠vel 1,2,3 basta colocar antes da frase `#` `##` ou `###` respectivamente\n",
    "\n",
    "> Mais sobre a sintaxe aqui: https://www.markdownguide.org/basic-syntax/\n",
    "\n",
    "texto = \"\"\"\n",
    "### T√≠tulo\n",
    "descri√ß√£o em **destaque** *aqui...*\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "display(Markdown(texto))\n",
    "\n",
    "E abaixo em forma de lista por exemplo\n",
    "\n",
    "Com isso, se pedirmos por exemplo pra LLM retornar uma lista ent√£o ficar√° mais apresent√°vel tamb√©m, j√° que ela ir√° retornar nesse formato com * no inicio de cada item, j√° que √© um padr√£o comum\n",
    "\n",
    "Nos projetos em que usamos os Streamlit talvez tenha notada que j√° ficou automaticamente dessem modo, pois a interface j√° interpreta corretamente markdown\n",
    "\n",
    "lista = \"\"\"\n",
    "* item\n",
    "* item\n",
    "* item\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(lista))\n",
    "\n",
    "### Finaliza√ß√£o do c√≥digo\n",
    "\n",
    "Copiamos do c√≥digo que fizemos antes, s√≥ mudando de infos[0].metadata para metadata\n",
    "\n",
    "e ao inv√©s do print() normal, vamos colocar no lugar display(Markdown()) para deixar mais apresent√°vel\n",
    "\n",
    "Como resposta, teremos:\n",
    "* Informa√ß√µes do v√≠deo - que s√£o os metadados: t√≠tulo, autor, data, URL.\n",
    "\n",
    "*  Sobre o que fala o v√≠deo - resumo de 1 frase, para contextualizarmos bem rapidamente\n",
    "\n",
    "* Temas - listagem dos principais temas desse v√≠deo\n",
    "\n",
    "* Resposta para a consulta - que √© a resposta para a consulta personalizada que fizemos, que pode ser uma pergunta ou instru√ß√£o\n",
    "\n",
    "Aqui voc√™ pode customizar √† vontade depois, deixar com as consultas que achar mais conveniente para o objetivo de sua aplica√ß√£o\n",
    "\n",
    "Voc√™ poderia tamb√©m separar em duas fun√ß√µes: uma para exibir junto as consultas fixas (informa√ß√µes do v√≠deo, resumo e temas) e outra para exibir a consulta personalizada, podendo deixar em formato de chat, igual feito no projeto 1 e 2.\n",
    "\n",
    "mas aqui estamos deixando mais simples e r√°pido, portanto ficou desse modo\n",
    "\n",
    "def interpret_video(url, query=\"liste os temas desse v√≠deo\", model_class=\"hf_hub\", language=\"pt\", translation=None):\n",
    "\n",
    "  try:\n",
    "    transcript, video_title = get_video_info(url, language, translation)\n",
    "\n",
    "    infos_video = f\"\"\"## Informa√ß√µes do v√≠deo\n",
    "\n",
    "    T√≠tulo: {video_title}\n",
    "    URL: {url}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    display(Markdown(infos_video))\n",
    "\n",
    "    chain = llm_chain(model_class)\n",
    "\n",
    "    t = \"\\n## Sobre o que fala o v√≠deo \\n\"\n",
    "    res = chain.invoke({\"transcricao\": transcript, \"consulta\": \"explique em 1 frase sobre o que fala esse v√≠deo. responda direto com a frase\"})\n",
    "    display(Markdown(t + res))\n",
    "\n",
    "    t = \"\\n## Temas \\n\"\n",
    "    res = chain.invoke({\"transcricao\": transcript, \"consulta\": \"lista os principais temas desse v√≠deo\"})\n",
    "    display(Markdown(t + res))\n",
    "\n",
    "    t = \"\\n## Resposta para a consulta \\n\"\n",
    "    res = chain.invoke({\"transcricao\": transcript, \"consulta\": query})\n",
    "    display(Markdown(t + res))\n",
    "\n",
    "  except Exception as e:\n",
    "    print(\"Erro ao carregar transcri√ß√£o\")\n",
    "    print(e)\n",
    "\n",
    "url_video = \"https://www.youtube.com/watch?v=rEE8ERGKsqo\" # @param {type:\"string\"}\n",
    "query_user = \"sumarize de forma clara de entender\" # @param {type:\"string\"}\n",
    "model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n",
    "language = [\"pt\", \"pt-BR\", \"en\"] # @param {type:\"string\"}\n",
    "\n",
    "interpret_video(url_video, query_user, model_class, language)\n",
    "\n",
    "url_video = \"https://www.youtube.com/watch?v=n9u-TITxwoM\" # @param {type:\"string\"}\n",
    "query_user = \"sumarize de forma clara de entender\" # @param {type:\"string\"}\n",
    "model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n",
    "language = [\"pt\", \"pt-BR\", \"en\"] # @param {type:\"string\"}\n",
    "\n",
    "interpret_video(url_video, query_user, model_class, language)\n",
    "\n",
    "url_video = \"https://www.youtube.com/watch?v=XXcHDy3QH-E\" # @param {type:\"string\"}\n",
    "query_user = \"sumarize de forma clara de entender\" # @param {type:\"string\"}\n",
    "model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n",
    "language = [\"pt\", \"pt-BR\", \"en\"] # @param {type:\"string\"}\n",
    "\n",
    "interpret_video(url_video, query_user, model_class, language)\n",
    "\n",
    "## Reconhecimento de fala\n",
    "\n",
    "Se o v√≠deo n√£o tiver uma transcri√ß√£o dispon√≠vel, ser√° necess√°rio gerar uma de forma autom√°tica utilizando um modelo de reconhecimento de fala, tamb√©m conhecido como Speech-to-Text (STT). Esses modelos convertem fala em texto a partir de arquivos de √°udio, permitindo que o conte√∫do do v√≠deo seja transcrito para ser processado pela nossa aplica√ß√£o de sumariza√ß√£o.\n",
    "\n",
    "Um exemplo popular de modelo de Speech-to-Text √© o [Whisper](https://openai.com/index/whisper/), da OpenAI, que oferece uma solu√ß√£o robusta para transcri√ß√£o autom√°tica. Os pre√ßos e detalhes sobre o uso desse modelo podem ser encontrados na p√°gina de \"[pricing](https://openai.com/api/pricing/)\" da OpenAI. No entanto, neste projeto, optamos por n√£o utiliz√°-lo, pois todos os v√≠deos testados j√° possu√≠am transcri√ß√µes, muitas vezes geradas automaticamente pelo YouTube. Essa abordagem economiza processamento, j√° que o foco principal foi a implementa√ß√£o de modelos de linguagem grandes (LLMs) e a explora√ß√£o de ferramentas do Langchain.\n",
    "\n",
    "* Se voc√™ precisar integrar um servi√ßo de transcri√ß√£o, a implementa√ß√£o √© simples, podendo ser feita com uma chamada de fun√ß√£o. A documenta√ß√£o do Langchain oferece instru√ß√µes detalhadas sobre como realizar essa integra√ß√£o aqui: https://js.langchain.com/v0.2/docs/integrations/document_loaders/file_loaders/openai_whisper_audio/.\n",
    "\n",
    "* Na verdade voc√™ tem a liberdade de escolher qualquer servi√ßo ou c√≥digo para realizar o reconhecimento de fala. Mesmo que o servi√ßo n√£o se integre diretamente ao Langchain, isso n√£o √© um problema, pois o essencial √© obter o texto final. No final, o que voc√™ obt√©m √© um texto normal, que pode ser armazenado em um arquivo de texto ou diretamente em uma vari√°vel.\n",
    "\n",
    "* Entretanto, se desejar aproveitar solu√ß√µes prontas dentro do Langchain, voc√™ pode utilizar o Whisper da OpenAI ou outras op√ß√µes como o [GoogleSpeechToText](https://python.langchain.com/v0.2/docs/integrations/document_loaders/google_speech_to_text/) e o [AssemblyAI Audio Transcripts](https://python.langchain.com/v0.2/docs/integrations/document_loaders/assemblyai/), que tamb√©m s√£o altamente eficazes e f√°ceis de integrar ao langchain.\n",
    "\n",
    "## Indo al√©m üöÄ\n",
    "\n",
    "1. Conforme mencionado anteriormente, para v√≠deos muito longos ou transcri√ß√µes maiores, pode ser necess√°rio dividir a transcri√ß√£o em partes menores e aplicar t√©cnicas de indexa√ß√£o e recupera√ß√£o utilizando RAG.\n",
    " * Isso √© essencial caso o v√≠deo que deseja processar possua horas por exemplo - talvez modelos muito modernos e com janela de contexto maior consigam lidar, mas mesmo que consigam pode ser interessante aplicar essas t√©cnicas, principalmente caso o que busque n√£o seja a sumariza√ß√£o mas sim algo pr√≥ximo do nosso projeto de perguntas e respostas com documentos, que ser√° visto no projeto 3.\n",
    " * Caso deseje seguir esse caminho, basta reutilizar o c√≥digo j√° pronto no projeto 3, que lida com esse tipo de segmenta√ß√£o e recupera√ß√£o de forma eficiente. Assim, voc√™ poder√° manter a integridade da transcri√ß√£o, otimizando o processamento e a gera√ß√£o de respostas mais precisas, mesmo com grandes volumes de dados.\n",
    "\n",
    "2. Como ideia adicional ou desafio: √© poss√≠vel personalizar este projeto para atender a outras necessidades, ou ainda usar a sumariza√ß√£o como uma etapa posterior dentro de uma pipeline maior.\n",
    "  * Um exemplo seria o uso da ferramenta [YouTubeSearchTool](https://python.langchain.com/v0.2/docs/integrations/tools/youtube//) do Langchain, que permite buscar automaticamente uma lista de URLs de v√≠deos no YouTube com base em um tema ou palavra-chave fornecida pelo usu√°rio.\n",
    "  * Nessa abordagem, voc√™ poderia implementar uma aplica√ß√£o que solicita um termo de busca e, em seguida, utiliza essa ferramenta para buscar v√≠deos relevantes, retornando em uma lista. Por fim, basta criar um la√ßo de repeti√ß√£o que execute a fun√ß√£o interpret_video (que criamos nesse projeto) para cada v√≠deo da lista, realizando assim a sumariza√ß√£o de m√∫ltiplos v√≠deos associados a um tema de forma automatizada.\n",
    "\n",
    "3. Al√©m disso, √© poss√≠vel integrar essa funcionalidade a uma interface interativa com ferramentas como Streamlit, que permite criar facilmente interfaces gr√°ficas.\n",
    " * Os m√©todos para essa integra√ß√£o est√£o descritos detalhadamente nos projetos 2 e 3, caso queira expandir a aplica√ß√£o.\n",
    " * Embora, neste exemplo, tenhamos optado por usar o Google Colab pela conveni√™ncia e para demonstrar a exibi√ß√£o com Markdown, a migra√ß√£o para uma interface mais robusta pode ser feita com facilidade e oferecer uma experi√™ncia mais fluida para o usu√°rio final caso deseje publicar essa aplica√ß√£o.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1oMsYRV_n8B8J6AYktIyhdYXX7QtnLxU0",
     "timestamp": 1724781537604
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
